{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Árvores de Decisão - Random Forest\n",
    "### Algoritmo árvores de decisão, melhorado com o random forest, usado em uma base de dados sobre o censo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando a biblioteca pandas do python\n",
    "import pandas as pd\n",
    "\n",
    "# atribuindo ao objeto \"dataframe\" todos os registros do banco de dados armazenados no arquivo \"census.csv\"\n",
    "dataframe = pd.read_csv(\"census.csv\", encoding = \"UTF-8\", sep = \",\")\n",
    "\n",
    "# separando os atributos previsores do \"dataframe\"\n",
    "previsores = dataframe.iloc[:,0:14].values\n",
    "\n",
    "# separando os atributos meta classe do \"dataframe\"\n",
    "meta_classe = dataframe.iloc[:, 14].values\n",
    "\n",
    "# importando a biblioteca sklearn do Python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# LabelEncoder é uma função responsável por  normalizar rótulos\n",
    "# o objetivo é transoformar variáveis categóricas em numéricas\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# importando a biblioteca sklearn do Python\n",
    "# função \"OneHotEncoder\" responsável por fazer a trasnformação em variáveis \"dummy\"\n",
    "\n",
    "# importando a biblioteca sklearn do Python\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# função \"ColumnTransformer\" responsável por definir quais colunas o objeto irá agir na alteração\n",
    "\n",
    "# criando o nosso objeto \"LabelEncoder_previsores\" para fazer a transformação dos campos de dados categóricos em numéricos\n",
    "LabelEncoder_previsores = LabelEncoder()\n",
    "\n",
    "# aplicando a transformação em todos os campos de dados categóricos usando o objeto criado e definido \"LabelEncoder_previsores\"\n",
    "previsores[:,1] = LabelEncoder_previsores.fit_transform(previsores[:,1])\n",
    "previsores[:,3] = LabelEncoder_previsores.fit_transform(previsores[:,3])\n",
    "previsores[:,5] = LabelEncoder_previsores.fit_transform(previsores[:,5])\n",
    "previsores[:,6] = LabelEncoder_previsores.fit_transform(previsores[:,6])\n",
    "previsores[:,7] = LabelEncoder_previsores.fit_transform(previsores[:,7])\n",
    "previsores[:,8] = LabelEncoder_previsores.fit_transform(previsores[:,8])\n",
    "previsores[:,9] = LabelEncoder_previsores.fit_transform(previsores[:,9])\n",
    "previsores[:,13] = LabelEncoder_previsores.fit_transform(previsores[:,13])\n",
    "\n",
    "# criando e configurando o objeto \"onehotencoder\" \n",
    "onehotencoder = ColumnTransformer(transformers = [(\"OneHot\", OneHotEncoder(), [1,3,5,6,7,8,9,13])], remainder = \"passthrough\")\n",
    "\n",
    "# transformando os atributos previsores em variáveis dummy usando o objeto \"onehotencoder\"\n",
    "previsores = onehotencoder.fit_transform(previsores).toarray()\n",
    "\n",
    "# importando a biblioteca sklearn do Python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# função \"StandardScaler\" responsável por fazer escalonamento de atributos\n",
    "\n",
    "# criando e configurando o objeto \"scaler\"\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# aplicando o escalonamento nos atributos previsores\n",
    "previsores = scaler.fit_transform(previsores)\n",
    "\n",
    "# importando a biblioteca sklearn do Python\n",
    "from sklearn.model_selection import train_test_split\n",
    "# a função \"train_test_split\" tem a função de separar modelos de treinamento e modelos de teste em uma base de dados\n",
    "\n",
    "# definir os modelos de treinamento e modelos de teste para os atributos previsores e meta_classe\n",
    "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, meta_classe, \n",
    "                                                                                              test_size = 0.15,\n",
    "                                                                                             random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando o pacote sklearn do python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# função \"RandomForestClassifier\" responsável por criar as diversas árvores de decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando o objeto \"classificaodor\", esse que receberá as árvores de decisão relativa a base de dados trabalhada\n",
    "classificador = RandomForestClassifier(n_estimators = 40, criterion = \"entropy\", random_state = 0)\n",
    "# nesse caso, foi especificado a criação de 10 árvores de decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='entropy', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=40,\n",
       "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fazendo o treinamento com a base de dados\n",
    "classificador.fit(previsores_treinamento, classe_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usando a base de dados de teste para analisar a capacidade de predição do algoritmo feito\n",
    "previsoes = classificador.predict(previsores_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**Saída de Dados (Legenda)**|\n",
    "|:---------------------------|\n",
    "|\"<=50K\" = renda menor que 50 mil|\n",
    "|\">50K\" = renda maior que 50 mil|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando o pacote sklearn do python\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "# \"confusion_matrix\" relaciona os dados originais com os dados previstos pelo algoritmo em uma matriz\n",
    "# \"accuracy_score\" exibe uma porcentagem de confiança no algoritmo, com base nos dados originais e nos dados previstos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usando \"accuracy_score\" para obter a precisão do algoritmo\n",
    "precisao = accuracy_score(classe_teste, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.847697031729785\n"
     ]
    }
   ],
   "source": [
    "# exibindo na saída de dados a porcentagem de acertos obtidos pelo algoritmo feito\n",
    "print(precisao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**Resultados do algoritmo**|\n",
    "|:--------------------------|\n",
    "|(labelencoder + onehotencoder + escalonamento) = **81.04%**|\n",
    "|[labelencoder + onehotencoder + escalonamento + Random Forest (40 árvores)] = **84.76%**|\n",
    "|(labelencoder + escalonamento) = |\n",
    "|(labelencoder + onehotencoder) = |\n",
    "|(labelencoder) = |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando a biblioteca pandas do python\n",
    "import pandas as pd\n",
    "\n",
    "# atribuindo ao objeto \"dataframe\" todos os registros do banco de dados armazenados no arquivo \"census.csv\"\n",
    "dataframe = pd.read_csv(\"census.csv\", encoding = \"UTF-8\", sep = \",\")\n",
    "\n",
    "# separando os atributos previsores do \"dataframe\"\n",
    "previsores = dataframe.iloc[:,0:14].values\n",
    "\n",
    "# separando os atributos meta classe do \"dataframe\"\n",
    "meta_classe = dataframe.iloc[:, 14].values\n",
    "\n",
    "# importando a biblioteca sklearn do Python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# LabelEncoder é uma função responsável por  normalizar rótulos\n",
    "# o objetivo é transoformar variáveis categóricas em numéricas\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# importando a biblioteca sklearn do Python\n",
    "# função \"OneHotEncoder\" responsável por fazer a trasnformação em variáveis \"dummy\"\n",
    "\n",
    "# importando a biblioteca sklearn do Python\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# função \"ColumnTransformer\" responsável por definir quais colunas o objeto irá agir na alteração\n",
    "\n",
    "# criando o nosso objeto \"LabelEncoder_previsores\" para fazer a transformação dos campos de dados categóricos em numéricos\n",
    "LabelEncoder_previsores = LabelEncoder()\n",
    "\n",
    "# aplicando a transformação em todos os campos de dados categóricos usando o objeto criado e definido \"LabelEncoder_previsores\"\n",
    "previsores[:,1] = LabelEncoder_previsores.fit_transform(previsores[:,1])\n",
    "previsores[:,3] = LabelEncoder_previsores.fit_transform(previsores[:,3])\n",
    "previsores[:,5] = LabelEncoder_previsores.fit_transform(previsores[:,5])\n",
    "previsores[:,6] = LabelEncoder_previsores.fit_transform(previsores[:,6])\n",
    "previsores[:,7] = LabelEncoder_previsores.fit_transform(previsores[:,7])\n",
    "previsores[:,8] = LabelEncoder_previsores.fit_transform(previsores[:,8])\n",
    "previsores[:,9] = LabelEncoder_previsores.fit_transform(previsores[:,9])\n",
    "previsores[:,13] = LabelEncoder_previsores.fit_transform(previsores[:,13])\n",
    "\n",
    "# criando e configurando o objeto \"onehotencoder\" \n",
    "#onehotencoder = ColumnTransformer(transformers = [(\"OneHot\", OneHotEncoder(), [1,3,5,6,7,8,9,13])], remainder = \"passthrough\")\n",
    "\n",
    "# transformando os atributos previsores em variáveis dummy usando o objeto \"onehotencoder\"\n",
    "#previsores = onehotencoder.fit_transform(previsores).toarray()\n",
    "\n",
    "# importando a biblioteca sklearn do Python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# função \"StandardScaler\" responsável por fazer escalonamento de atributos\n",
    "\n",
    "# criando e configurando o objeto \"scaler\"\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# aplicando o escalonamento nos atributos previsores\n",
    "previsores = scaler.fit_transform(previsores)\n",
    "\n",
    "## importando a biblioteca sklearn do Python\n",
    "from sklearn.model_selection import train_test_split\n",
    "# a função \"train_test_split\" tem a função de separar modelos de treinamento e modelos de teste em uma base de dados\n",
    "\n",
    "# definir os modelos de treinamento e modelos de teste para os atributos previsores e meta_classe\n",
    "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, meta_classe, \n",
    "                                                                                              test_size = 0.15,\n",
    "                                                                                             random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8483111566018424\n"
     ]
    }
   ],
   "source": [
    "# criando o objeto \"classificaodor\", esse que receberá as árvores de decisão relativa a base de dados trabalhada\n",
    "classificador = RandomForestClassifier(n_estimators = 40, criterion = \"entropy\", random_state = 0)\n",
    "# nesse caso, foi especificado a criação de 10 árvores de decisão\n",
    "\n",
    "# fazendo o treinamento com a base de dados\n",
    "classificador.fit(previsores_treinamento, classe_treinamento)\n",
    "\n",
    "# usando a base de dados de teste para analisar a capacidade de predição do algoritmo feito\n",
    "previsoes = classificador.predict(previsores_teste)\n",
    "\n",
    "# usando \"accuracy_score\" para obter a precisão do algoritmo\n",
    "precisao = accuracy_score(classe_teste, previsoes)\n",
    "\n",
    "# exibindo na saída de dados a porcentagem de acertos obtidos pelo algoritmo feito\n",
    "print(precisao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**Resultados do algoritmo**|\n",
    "|:--------------------------|\n",
    "|(labelencoder + onehotencoder + escalonamento) = **81.04%**|\n",
    "|[labelencoder + onehotencoder + escalonamento + Random Forest (40 árvores)] = **84.76%**|\n",
    "|(labelencoder + escalonamento) = **81.29%**|\n",
    "|[labelencoder + escalonamento + Random Forest (40 árvores)] = **84.83%**|\n",
    "|(labelencoder + onehotencoder) = |\n",
    "|(labelencoder) = |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando a biblioteca pandas do python\n",
    "import pandas as pd\n",
    "\n",
    "# atribuindo ao objeto \"dataframe\" todos os registros do banco de dados armazenados no arquivo \"census.csv\"\n",
    "dataframe = pd.read_csv(\"census.csv\", encoding = \"UTF-8\", sep = \",\")\n",
    "\n",
    "# separando os atributos previsores do \"dataframe\"\n",
    "previsores = dataframe.iloc[:,0:14].values\n",
    "\n",
    "# separando os atributos meta classe do \"dataframe\"\n",
    "meta_classe = dataframe.iloc[:, 14].values\n",
    "\n",
    "# importando a biblioteca sklearn do Python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# LabelEncoder é uma função responsável por  normalizar rótulos\n",
    "# o objetivo é transoformar variáveis categóricas em numéricas\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# importando a biblioteca sklearn do Python\n",
    "# função \"OneHotEncoder\" responsável por fazer a trasnformação em variáveis \"dummy\"\n",
    "\n",
    "# importando a biblioteca sklearn do Python\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# função \"ColumnTransformer\" responsável por definir quais colunas o objeto irá agir na alteração\n",
    "\n",
    "# criando o nosso objeto \"LabelEncoder_previsores\" para fazer a transformação dos campos de dados categóricos em numéricos\n",
    "LabelEncoder_previsores = LabelEncoder()\n",
    "\n",
    "# aplicando a transformação em todos os campos de dados categóricos usando o objeto criado e definido \"LabelEncoder_previsores\"\n",
    "previsores[:,1] = LabelEncoder_previsores.fit_transform(previsores[:,1])\n",
    "previsores[:,3] = LabelEncoder_previsores.fit_transform(previsores[:,3])\n",
    "previsores[:,5] = LabelEncoder_previsores.fit_transform(previsores[:,5])\n",
    "previsores[:,6] = LabelEncoder_previsores.fit_transform(previsores[:,6])\n",
    "previsores[:,7] = LabelEncoder_previsores.fit_transform(previsores[:,7])\n",
    "previsores[:,8] = LabelEncoder_previsores.fit_transform(previsores[:,8])\n",
    "previsores[:,9] = LabelEncoder_previsores.fit_transform(previsores[:,9])\n",
    "previsores[:,13] = LabelEncoder_previsores.fit_transform(previsores[:,13])\n",
    "\n",
    "# criando e configurando o objeto \"onehotencoder\" \n",
    "onehotencoder = ColumnTransformer(transformers = [(\"OneHot\", OneHotEncoder(), [1,3,5,6,7,8,9,13])], remainder = \"passthrough\")\n",
    "\n",
    "# transformando os atributos previsores em variáveis dummy usando o objeto \"onehotencoder\"\n",
    "previsores = onehotencoder.fit_transform(previsores).toarray()\n",
    "\n",
    "# importando a biblioteca sklearn do Python\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "# função \"StandardScaler\" responsável por fazer escalonamento de atributos\n",
    "\n",
    "# criando e configurando o objeto \"scaler\"\n",
    "#scaler = StandardScaler()\n",
    "\n",
    "# aplicando o escalonamento nos atributos previsores\n",
    "#previsores = scaler.fit_transform(previsores)\n",
    "\n",
    "## importando a biblioteca sklearn do Python\n",
    "from sklearn.model_selection import train_test_split\n",
    "# a função \"train_test_split\" tem a função de separar modelos de treinamento e modelos de teste em uma base de dados\n",
    "\n",
    "# definir os modelos de treinamento e modelos de teste para os atributos previsores e meta_classe\n",
    "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, meta_classe, \n",
    "                                                                                              test_size = 0.15,\n",
    "                                                                                             random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8489252814738997\n"
     ]
    }
   ],
   "source": [
    "# criando o objeto \"classificaodor\", esse que receberá as árvores de decisão relativa a base de dados trabalhada\n",
    "classificador = RandomForestClassifier(n_estimators = 40, criterion = \"entropy\", random_state = 0)\n",
    "# nesse caso, foi especificado a criação de 10 árvores de decisão\n",
    "\n",
    "# fazendo o treinamento com a base de dados\n",
    "classificador.fit(previsores_treinamento, classe_treinamento)\n",
    "\n",
    "# usando a base de dados de teste para analisar a capacidade de predição do algoritmo feito\n",
    "previsoes = classificador.predict(previsores_teste)\n",
    "\n",
    "# usando \"accuracy_score\" para obter a precisão do algoritmo\n",
    "precisao = accuracy_score(classe_teste, previsoes)\n",
    "\n",
    "# exibindo na saída de dados a porcentagem de acertos obtidos pelo algoritmo feito\n",
    "print(precisao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**Resultados do algoritmo**|\n",
    "|:--------------------------|\n",
    "|(labelencoder + onehotencoder + escalonamento) = **81.04%**|\n",
    "|[labelencoder + onehotencoder + escalonamento + Random Forest (40 árvores)] = **84.76%**|\n",
    "|(labelencoder + escalonamento) = **81.29%**|\n",
    "|[labelencoder + escalonamento + Random Forest (40 árvores)] = **84.83%**|\n",
    "|(labelencoder + onehotencoder) = **81.02%**|\n",
    "|[labelencoder + onehotencoder + Random Forest (40 árvores)] = **84.89%**|\n",
    "|(labelencoder) = |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando a biblioteca pandas do python\n",
    "import pandas as pd\n",
    "\n",
    "# atribuindo ao objeto \"dataframe\" todos os registros do banco de dados armazenados no arquivo \"census.csv\"\n",
    "dataframe = pd.read_csv(\"census.csv\", encoding = \"UTF-8\", sep = \",\")\n",
    "\n",
    "# separando os atributos previsores do \"dataframe\"\n",
    "previsores = dataframe.iloc[:,0:14].values\n",
    "\n",
    "# separando os atributos meta classe do \"dataframe\"\n",
    "meta_classe = dataframe.iloc[:, 14].values\n",
    "\n",
    "# importando a biblioteca sklearn do Python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# LabelEncoder é uma função responsável por  normalizar rótulos\n",
    "# o objetivo é transoformar variáveis categóricas em numéricas\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# importando a biblioteca sklearn do Python\n",
    "# função \"OneHotEncoder\" responsável por fazer a trasnformação em variáveis \"dummy\"\n",
    "\n",
    "# importando a biblioteca sklearn do Python\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# função \"ColumnTransformer\" responsável por definir quais colunas o objeto irá agir na alteração\n",
    "\n",
    "# criando o nosso objeto \"LabelEncoder_previsores\" para fazer a transformação dos campos de dados categóricos em numéricos\n",
    "LabelEncoder_previsores = LabelEncoder()\n",
    "\n",
    "# aplicando a transformação em todos os campos de dados categóricos usando o objeto criado e definido \"LabelEncoder_previsores\"\n",
    "previsores[:,1] = LabelEncoder_previsores.fit_transform(previsores[:,1])\n",
    "previsores[:,3] = LabelEncoder_previsores.fit_transform(previsores[:,3])\n",
    "previsores[:,5] = LabelEncoder_previsores.fit_transform(previsores[:,5])\n",
    "previsores[:,6] = LabelEncoder_previsores.fit_transform(previsores[:,6])\n",
    "previsores[:,7] = LabelEncoder_previsores.fit_transform(previsores[:,7])\n",
    "previsores[:,8] = LabelEncoder_previsores.fit_transform(previsores[:,8])\n",
    "previsores[:,9] = LabelEncoder_previsores.fit_transform(previsores[:,9])\n",
    "previsores[:,13] = LabelEncoder_previsores.fit_transform(previsores[:,13])\n",
    "\n",
    "# criando e configurando o objeto \"onehotencoder\" \n",
    "#onehotencoder = ColumnTransformer(transformers = [(\"OneHot\", OneHotEncoder(), [1,3,5,6,7,8,9,13])], remainder = \"passthrough\")\n",
    "\n",
    "# transformando os atributos previsores em variáveis dummy usando o objeto \"onehotencoder\"\n",
    "#previsores = onehotencoder.fit_transform(previsores).toarray()\n",
    "\n",
    "# importando a biblioteca sklearn do Python\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "# função \"StandardScaler\" responsável por fazer escalonamento de atributos\n",
    "\n",
    "# criando e configurando o objeto \"scaler\"\n",
    "#scaler = StandardScaler()\n",
    "\n",
    "# aplicando o escalonamento nos atributos previsores\n",
    "#previsores = scaler.fit_transform(previsores)\n",
    "\n",
    "## importando a biblioteca sklearn do Python\n",
    "from sklearn.model_selection import train_test_split\n",
    "# a função \"train_test_split\" tem a função de separar modelos de treinamento e modelos de teste em uma base de dados\n",
    "\n",
    "# definir os modelos de treinamento e modelos de teste para os atributos previsores e meta_classe\n",
    "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, meta_classe, \n",
    "                                                                                              test_size = 0.15,\n",
    "                                                                                             random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8481064483111566\n"
     ]
    }
   ],
   "source": [
    "# criando o objeto \"classificaodor\", esse que receberá as árvores de decisão relativa a base de dados trabalhada\n",
    "classificador = RandomForestClassifier(n_estimators = 40, criterion = \"entropy\", random_state = 0)\n",
    "# nesse caso, foi especificado a criação de 10 árvores de decisão\n",
    "\n",
    "# fazendo o treinamento com a base de dados\n",
    "classificador.fit(previsores_treinamento, classe_treinamento)\n",
    "\n",
    "# usando a base de dados de teste para analisar a capacidade de predição do algoritmo feito\n",
    "previsoes = classificador.predict(previsores_teste)\n",
    "\n",
    "# usando \"accuracy_score\" para obter a precisão do algoritmo\n",
    "precisao = accuracy_score(classe_teste, previsoes)\n",
    "\n",
    "# exibindo na saída de dados a porcentagem de acertos obtidos pelo algoritmo feito\n",
    "print(precisao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**Resultados do algoritmo**|\n",
    "|:--------------------------|\n",
    "|(labelencoder + onehotencoder + escalonamento) = **81.04%**|\n",
    "|[labelencoder + onehotencoder + escalonamento + Random Forest (40 árvores)] = **84.76%**|\n",
    "|(labelencoder + escalonamento) = **81.29%**|\n",
    "|[labelencoder + escalonamento + Random Forest (40 árvores)] = **84.83%**|\n",
    "|(labelencoder + onehotencoder) = **81.02%**|\n",
    "|[labelencoder + onehotencoder + Random Forest (40 árvores)] = **84.89%**|\n",
    "|(labelencoder) = **81.29%**|\n",
    "|[labelencoder + Random Forest (40 árvores)] = **84.81%**|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alguma dúvida? Entre em contato comigo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Me envie um e-mail](mailto:alysson.barbosa@ee.ufcg.edu.br)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
