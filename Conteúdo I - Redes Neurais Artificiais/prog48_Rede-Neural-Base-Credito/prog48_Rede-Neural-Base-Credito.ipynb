{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rede Neural Base de Crédito\n",
    "#### Criando uma rede neural para resolver o problema da base de crédito."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando a biblioteca pandas do python\n",
    "import pandas as pd\n",
    "\n",
    "# importando a base de dados para o objeto \"dataframe\"\n",
    "dataframe = pd.read_csv('Dados de Credito.csv', encoding = 'utf-8', sep = ',')\n",
    "\n",
    "# substituindo valores inconsistentes no campo idades pela média das idades consistentes\n",
    "dataframe.loc[dataframe.age < 0, 'age'] = 40.92\n",
    "\n",
    "# separando os atributos previsores do meta classe\n",
    "previsores = dataframe.iloc[:, 1:4].values\n",
    "classe = dataframe.iloc[:, 4].values\n",
    "\n",
    "# importando a biblioteca sklearn do python\n",
    "from sklearn.impute import SimpleImputer\n",
    "# função \"SimpleImputer\" responsável por corrigir valores faltantes na base de dados\n",
    "\n",
    "# importando a biblioteca numpy do python\n",
    "import numpy as np\n",
    "\n",
    "# criando o objeto \"imputer\"\n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy = \"mean\")\n",
    "\n",
    "# fazendo o treinamento com a base de dados para correção de valores faltantes\n",
    "imputer = imputer.fit(previsores[:,0:3])\n",
    "\n",
    "# corrigindo os valores faltantes usando o objeto criado \"imputer\"\n",
    "previsores[:, 0:3] = imputer.transform(previsores[:, 0:3])\n",
    "\n",
    "# importando a biblioteca sklearn do python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# a função \"StandardScaler\" tem a função de escalonas toda a base de dados para corrigir a discrepância\n",
    "# dos valores\n",
    "\n",
    "# criando o objeto \"scaler\"\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# escalonando todos os atributos para auxiliar no cálculo de distâncias euclidianas\n",
    "previsores = scaler.fit_transform(previsores)\n",
    "\n",
    "# importando a biblioteca sklearn do python\n",
    "from sklearn.model_selection import train_test_split\n",
    "# a função \"train_test_split\" tem a importância de separar modelos de treinamento e modelos de teste\n",
    "# em uma base de dados\n",
    "\n",
    "# criando um modelo de treinamento e um modelo de teste\n",
    "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores,\n",
    "                                                                                             classe,\n",
    "                                                                                             test_size = 0.25,\n",
    "                                                                                             random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando uma rede neural usando a linguagem de programação Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando a biblioteca sklearn do python\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# 'MLPClassifier' é usado para criar uma rede neural\n",
    "# Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando e definindo um 'classificador' para definir os parâmetros da rede neural\n",
    "# verbose mostra na tela os dados em cada época de treinamento\n",
    "# max_iter = números de repetições, ou seja, quantidade de vezes que o algoritmo será treinado\n",
    "# tol = ajusta os pesos com passos pequenos\n",
    "# solver é a otimização utilizada, 'adam' é um melhoramente do método gradiente descente\n",
    "# activation = função de ativação\n",
    "# hidden_layer_sizes = quantidade de neurônios em cada camada oculta\n",
    "# learning_rate = indica o método de ajuste de pesos, em 'constant' o treinamento para quando o peso não muda por duas vezes consecutivas e\n",
    "# 'adaptative' muda a taxa de aprendizado quando o erro não diminui\n",
    "# muda a taxa de aprendizagem\n",
    "classificador = MLPClassifier(verbose = True, max_iter = 1000, tol = 0.0000010, solver = 'adam', hidden_layer_sizes = (100, 100),\n",
    "                              activation = 'relu', learning_rate = 'constant', random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.68988648\n",
      "Iteration 2, loss = 0.48556666\n",
      "Iteration 3, loss = 0.36127704\n",
      "Iteration 4, loss = 0.28295103\n",
      "Iteration 5, loss = 0.22659304\n",
      "Iteration 6, loss = 0.18679249\n",
      "Iteration 7, loss = 0.15799548\n",
      "Iteration 8, loss = 0.13670874\n",
      "Iteration 9, loss = 0.12083605\n",
      "Iteration 10, loss = 0.10894405\n",
      "Iteration 11, loss = 0.09896763\n",
      "Iteration 12, loss = 0.09104808\n",
      "Iteration 13, loss = 0.08438450\n",
      "Iteration 14, loss = 0.07853949\n",
      "Iteration 15, loss = 0.07383596\n",
      "Iteration 16, loss = 0.06930532\n",
      "Iteration 17, loss = 0.06540275\n",
      "Iteration 18, loss = 0.06162943\n",
      "Iteration 19, loss = 0.05837095\n",
      "Iteration 20, loss = 0.05535257\n",
      "Iteration 21, loss = 0.05259643\n",
      "Iteration 22, loss = 0.04963190\n",
      "Iteration 23, loss = 0.04767244\n",
      "Iteration 24, loss = 0.04536270\n",
      "Iteration 25, loss = 0.04290771\n",
      "Iteration 26, loss = 0.04104809\n",
      "Iteration 27, loss = 0.03941188\n",
      "Iteration 28, loss = 0.03812440\n",
      "Iteration 29, loss = 0.03621617\n",
      "Iteration 30, loss = 0.03487377\n",
      "Iteration 31, loss = 0.03334103\n",
      "Iteration 32, loss = 0.03227516\n",
      "Iteration 33, loss = 0.03077099\n",
      "Iteration 34, loss = 0.03044285\n",
      "Iteration 35, loss = 0.02894901\n",
      "Iteration 36, loss = 0.02837966\n",
      "Iteration 37, loss = 0.02718030\n",
      "Iteration 38, loss = 0.02621757\n",
      "Iteration 39, loss = 0.02556714\n",
      "Iteration 40, loss = 0.02494552\n",
      "Iteration 41, loss = 0.02443730\n",
      "Iteration 42, loss = 0.02377040\n",
      "Iteration 43, loss = 0.02309035\n",
      "Iteration 44, loss = 0.02266638\n",
      "Iteration 45, loss = 0.02195201\n",
      "Iteration 46, loss = 0.02176242\n",
      "Iteration 47, loss = 0.02072020\n",
      "Iteration 48, loss = 0.02028846\n",
      "Iteration 49, loss = 0.01985404\n",
      "Iteration 50, loss = 0.01938475\n",
      "Iteration 51, loss = 0.01901888\n",
      "Iteration 52, loss = 0.01836467\n",
      "Iteration 53, loss = 0.01827018\n",
      "Iteration 54, loss = 0.01754936\n",
      "Iteration 55, loss = 0.01739855\n",
      "Iteration 56, loss = 0.01701003\n",
      "Iteration 57, loss = 0.01657189\n",
      "Iteration 58, loss = 0.01640367\n",
      "Iteration 59, loss = 0.01623013\n",
      "Iteration 60, loss = 0.01604531\n",
      "Iteration 61, loss = 0.01537616\n",
      "Iteration 62, loss = 0.01531271\n",
      "Iteration 63, loss = 0.01474401\n",
      "Iteration 64, loss = 0.01511808\n",
      "Iteration 65, loss = 0.01451266\n",
      "Iteration 66, loss = 0.01414915\n",
      "Iteration 67, loss = 0.01399282\n",
      "Iteration 68, loss = 0.01381522\n",
      "Iteration 69, loss = 0.01334915\n",
      "Iteration 70, loss = 0.01341423\n",
      "Iteration 71, loss = 0.01295147\n",
      "Iteration 72, loss = 0.01293755\n",
      "Iteration 73, loss = 0.01261024\n",
      "Iteration 74, loss = 0.01219953\n",
      "Iteration 75, loss = 0.01222676\n",
      "Iteration 76, loss = 0.01186906\n",
      "Iteration 77, loss = 0.01192047\n",
      "Iteration 78, loss = 0.01183040\n",
      "Iteration 79, loss = 0.01154531\n",
      "Iteration 80, loss = 0.01130560\n",
      "Iteration 81, loss = 0.01112021\n",
      "Iteration 82, loss = 0.01085059\n",
      "Iteration 83, loss = 0.01086736\n",
      "Iteration 84, loss = 0.01056704\n",
      "Iteration 85, loss = 0.01048952\n",
      "Iteration 86, loss = 0.01039213\n",
      "Iteration 87, loss = 0.01002086\n",
      "Iteration 88, loss = 0.01019188\n",
      "Iteration 89, loss = 0.01001885\n",
      "Iteration 90, loss = 0.00950635\n",
      "Iteration 91, loss = 0.00965486\n",
      "Iteration 92, loss = 0.00943404\n",
      "Iteration 93, loss = 0.00938897\n",
      "Iteration 94, loss = 0.00908896\n",
      "Iteration 95, loss = 0.00922147\n",
      "Iteration 96, loss = 0.00916729\n",
      "Iteration 97, loss = 0.00886711\n",
      "Iteration 98, loss = 0.00921630\n",
      "Iteration 99, loss = 0.00860519\n",
      "Iteration 100, loss = 0.00845262\n",
      "Iteration 101, loss = 0.00836553\n",
      "Iteration 102, loss = 0.00868928\n",
      "Iteration 103, loss = 0.00836316\n",
      "Iteration 104, loss = 0.00821560\n",
      "Iteration 105, loss = 0.00815687\n",
      "Iteration 106, loss = 0.00784564\n",
      "Iteration 107, loss = 0.00775407\n",
      "Iteration 108, loss = 0.00795428\n",
      "Iteration 109, loss = 0.00768388\n",
      "Iteration 110, loss = 0.00762858\n",
      "Iteration 111, loss = 0.00748702\n",
      "Iteration 112, loss = 0.00794915\n",
      "Iteration 113, loss = 0.00746862\n",
      "Iteration 114, loss = 0.00706821\n",
      "Iteration 115, loss = 0.00739604\n",
      "Iteration 116, loss = 0.00726846\n",
      "Iteration 117, loss = 0.00818283\n",
      "Iteration 118, loss = 0.00748376\n",
      "Iteration 119, loss = 0.00729416\n",
      "Iteration 120, loss = 0.00691536\n",
      "Iteration 121, loss = 0.00671579\n",
      "Iteration 122, loss = 0.00658570\n",
      "Iteration 123, loss = 0.00659494\n",
      "Iteration 124, loss = 0.00631149\n",
      "Iteration 125, loss = 0.00616959\n",
      "Iteration 126, loss = 0.00634269\n",
      "Iteration 127, loss = 0.00619536\n",
      "Iteration 128, loss = 0.00610012\n",
      "Iteration 129, loss = 0.00611143\n",
      "Iteration 130, loss = 0.00584203\n",
      "Iteration 131, loss = 0.00592567\n",
      "Iteration 132, loss = 0.00634275\n",
      "Iteration 133, loss = 0.00581331\n",
      "Iteration 134, loss = 0.00577661\n",
      "Iteration 135, loss = 0.00566466\n",
      "Iteration 136, loss = 0.00570217\n",
      "Iteration 137, loss = 0.00575041\n",
      "Iteration 138, loss = 0.00560084\n",
      "Iteration 139, loss = 0.00552545\n",
      "Iteration 140, loss = 0.00559007\n",
      "Iteration 141, loss = 0.00544561\n",
      "Iteration 142, loss = 0.00534915\n",
      "Iteration 143, loss = 0.00518132\n",
      "Iteration 144, loss = 0.00520229\n",
      "Iteration 145, loss = 0.00552361\n",
      "Iteration 146, loss = 0.00572723\n",
      "Iteration 147, loss = 0.00480287\n",
      "Iteration 148, loss = 0.00509112\n",
      "Iteration 149, loss = 0.00485938\n",
      "Iteration 150, loss = 0.00481367\n",
      "Iteration 151, loss = 0.00479705\n",
      "Iteration 152, loss = 0.00460118\n",
      "Iteration 153, loss = 0.00457578\n",
      "Iteration 154, loss = 0.00457180\n",
      "Iteration 155, loss = 0.00497180\n",
      "Iteration 156, loss = 0.00468319\n",
      "Iteration 157, loss = 0.00499678\n",
      "Iteration 158, loss = 0.00471356\n",
      "Iteration 159, loss = 0.00434481\n",
      "Iteration 160, loss = 0.00450097\n",
      "Iteration 161, loss = 0.00435043\n",
      "Iteration 162, loss = 0.00424932\n",
      "Iteration 163, loss = 0.00424192\n",
      "Iteration 164, loss = 0.00413820\n",
      "Iteration 165, loss = 0.00438649\n",
      "Iteration 166, loss = 0.00439371\n",
      "Iteration 167, loss = 0.00405655\n",
      "Iteration 168, loss = 0.00432061\n",
      "Iteration 169, loss = 0.00418238\n",
      "Iteration 170, loss = 0.00411898\n",
      "Iteration 171, loss = 0.00400368\n",
      "Iteration 172, loss = 0.00410008\n",
      "Iteration 173, loss = 0.00389525\n",
      "Iteration 174, loss = 0.00375122\n",
      "Iteration 175, loss = 0.00375816\n",
      "Iteration 176, loss = 0.00374795\n",
      "Iteration 177, loss = 0.00371053\n",
      "Iteration 178, loss = 0.00374409\n",
      "Iteration 179, loss = 0.00368702\n",
      "Iteration 180, loss = 0.00416373\n",
      "Iteration 181, loss = 0.00369764\n",
      "Iteration 182, loss = 0.00370408\n",
      "Iteration 183, loss = 0.00361431\n",
      "Iteration 184, loss = 0.00353292\n",
      "Iteration 185, loss = 0.00349802\n",
      "Iteration 186, loss = 0.00346534\n",
      "Iteration 187, loss = 0.00363131\n",
      "Iteration 188, loss = 0.00351284\n",
      "Iteration 189, loss = 0.00329194\n",
      "Iteration 190, loss = 0.00335681\n",
      "Iteration 191, loss = 0.00325443\n",
      "Iteration 192, loss = 0.00337229\n",
      "Iteration 193, loss = 0.00336408\n",
      "Iteration 194, loss = 0.00346464\n",
      "Iteration 195, loss = 0.00325788\n",
      "Iteration 196, loss = 0.00353982\n",
      "Iteration 197, loss = 0.00337978\n",
      "Iteration 198, loss = 0.00312322\n",
      "Iteration 199, loss = 0.00330688\n",
      "Iteration 200, loss = 0.00290278\n",
      "Iteration 201, loss = 0.00327006\n",
      "Iteration 202, loss = 0.00317065\n",
      "Iteration 203, loss = 0.00305660\n",
      "Iteration 204, loss = 0.00294658\n",
      "Iteration 205, loss = 0.00290362\n",
      "Iteration 206, loss = 0.00290381\n",
      "Iteration 207, loss = 0.00294453\n",
      "Iteration 208, loss = 0.00293371\n",
      "Iteration 209, loss = 0.00299642\n",
      "Iteration 210, loss = 0.00280380\n",
      "Iteration 211, loss = 0.00273859\n",
      "Iteration 212, loss = 0.00278274\n",
      "Iteration 213, loss = 0.00285487\n",
      "Iteration 214, loss = 0.00274706\n",
      "Iteration 215, loss = 0.00279184\n",
      "Iteration 216, loss = 0.00274852\n",
      "Iteration 217, loss = 0.00272710\n",
      "Iteration 218, loss = 0.00285398\n",
      "Iteration 219, loss = 0.00264779\n",
      "Iteration 220, loss = 0.00266259\n",
      "Iteration 221, loss = 0.00254548\n",
      "Iteration 222, loss = 0.00247911\n",
      "Iteration 223, loss = 0.00245487\n",
      "Iteration 224, loss = 0.00250802\n",
      "Iteration 225, loss = 0.00261422\n",
      "Iteration 226, loss = 0.00256787\n",
      "Iteration 227, loss = 0.00280991\n",
      "Iteration 228, loss = 0.00288609\n",
      "Iteration 229, loss = 0.00273040\n",
      "Iteration 230, loss = 0.00292999\n",
      "Iteration 231, loss = 0.00238509\n",
      "Iteration 232, loss = 0.00246951\n",
      "Iteration 233, loss = 0.00269377\n",
      "Iteration 234, loss = 0.00224375\n",
      "Iteration 235, loss = 0.00287830\n",
      "Iteration 236, loss = 0.00292112\n",
      "Iteration 237, loss = 0.00238943\n",
      "Iteration 238, loss = 0.00231524\n",
      "Iteration 239, loss = 0.00240339\n",
      "Iteration 240, loss = 0.00256399\n",
      "Iteration 241, loss = 0.00241461\n",
      "Iteration 242, loss = 0.00209003\n",
      "Iteration 243, loss = 0.00244937\n",
      "Iteration 244, loss = 0.00221512\n",
      "Iteration 245, loss = 0.00229994\n",
      "Iteration 246, loss = 0.00231094\n",
      "Iteration 247, loss = 0.00212318\n",
      "Iteration 248, loss = 0.00209415\n",
      "Iteration 249, loss = 0.00202430\n",
      "Iteration 250, loss = 0.00205071\n",
      "Iteration 251, loss = 0.00240133\n",
      "Iteration 252, loss = 0.00200843\n",
      "Iteration 253, loss = 0.00200750\n",
      "Iteration 254, loss = 0.00213091\n",
      "Iteration 255, loss = 0.00196222\n",
      "Iteration 256, loss = 0.00196488\n",
      "Iteration 257, loss = 0.00196932\n",
      "Iteration 258, loss = 0.00188458\n",
      "Iteration 259, loss = 0.00199510\n",
      "Iteration 260, loss = 0.00200546\n",
      "Iteration 261, loss = 0.00203710\n",
      "Iteration 262, loss = 0.00182503\n",
      "Iteration 263, loss = 0.00216321\n",
      "Iteration 264, loss = 0.00180767\n",
      "Iteration 265, loss = 0.00184236\n",
      "Iteration 266, loss = 0.00202991\n",
      "Iteration 267, loss = 0.00177935\n",
      "Iteration 268, loss = 0.00172432\n",
      "Iteration 269, loss = 0.00174872\n",
      "Iteration 270, loss = 0.00179885\n",
      "Iteration 271, loss = 0.00181108\n",
      "Iteration 272, loss = 0.00168285\n",
      "Iteration 273, loss = 0.00186219\n",
      "Iteration 274, loss = 0.00172306\n",
      "Iteration 275, loss = 0.00169147\n",
      "Iteration 276, loss = 0.00164074\n",
      "Iteration 277, loss = 0.00182611\n",
      "Iteration 278, loss = 0.00164459\n",
      "Iteration 279, loss = 0.00178091\n",
      "Iteration 280, loss = 0.00173944\n",
      "Iteration 281, loss = 0.00155953\n",
      "Iteration 282, loss = 0.00168797\n",
      "Iteration 283, loss = 0.00159273\n",
      "Iteration 284, loss = 0.00157737\n",
      "Iteration 285, loss = 0.00157950\n",
      "Iteration 286, loss = 0.00155266\n",
      "Iteration 287, loss = 0.00184423\n",
      "Iteration 288, loss = 0.00168653\n",
      "Iteration 289, loss = 0.00179389\n",
      "Iteration 290, loss = 0.00161493\n",
      "Iteration 291, loss = 0.00174341\n",
      "Iteration 292, loss = 0.00146276\n",
      "Iteration 293, loss = 0.00158674\n",
      "Iteration 294, loss = 0.00153530\n",
      "Iteration 295, loss = 0.00161709\n",
      "Iteration 296, loss = 0.00155811\n",
      "Iteration 297, loss = 0.00143261\n",
      "Iteration 298, loss = 0.00155593\n",
      "Iteration 299, loss = 0.00142418\n",
      "Iteration 300, loss = 0.00146748\n",
      "Iteration 301, loss = 0.00146189\n",
      "Iteration 302, loss = 0.00136333\n",
      "Iteration 303, loss = 0.00135511\n",
      "Iteration 304, loss = 0.00141395\n",
      "Iteration 305, loss = 0.00136323\n",
      "Iteration 306, loss = 0.00131147\n",
      "Iteration 307, loss = 0.00137576\n",
      "Iteration 308, loss = 0.00126059\n",
      "Iteration 309, loss = 0.00143247\n",
      "Iteration 310, loss = 0.00142932\n",
      "Iteration 311, loss = 0.00146396\n",
      "Iteration 312, loss = 0.00150430\n",
      "Iteration 313, loss = 0.00142938\n",
      "Iteration 314, loss = 0.00133324\n",
      "Iteration 315, loss = 0.00121129\n",
      "Iteration 316, loss = 0.00141692\n",
      "Iteration 317, loss = 0.00125801\n",
      "Iteration 318, loss = 0.00130363\n",
      "Iteration 319, loss = 0.00153037\n",
      "Iteration 320, loss = 0.00132079\n",
      "Iteration 321, loss = 0.00132659\n",
      "Iteration 322, loss = 0.00139178\n",
      "Iteration 323, loss = 0.00122626\n",
      "Iteration 324, loss = 0.00114314\n",
      "Iteration 325, loss = 0.00127616\n",
      "Iteration 326, loss = 0.00118099\n",
      "Iteration 327, loss = 0.00119519\n",
      "Iteration 328, loss = 0.00127133\n",
      "Iteration 329, loss = 0.00116003\n",
      "Iteration 330, loss = 0.00118280\n",
      "Iteration 331, loss = 0.00118221\n",
      "Iteration 332, loss = 0.00114518\n",
      "Iteration 333, loss = 0.00108848\n",
      "Iteration 334, loss = 0.00114259\n",
      "Iteration 335, loss = 0.00109852\n",
      "Iteration 336, loss = 0.00111809\n",
      "Iteration 337, loss = 0.00127084\n",
      "Iteration 338, loss = 0.00152494\n",
      "Iteration 339, loss = 0.00138752\n",
      "Iteration 340, loss = 0.00137026\n",
      "Iteration 341, loss = 0.00125888\n",
      "Iteration 342, loss = 0.00110233\n",
      "Iteration 343, loss = 0.00107564\n",
      "Iteration 344, loss = 0.00112245\n",
      "Iteration 345, loss = 0.00125563\n",
      "Iteration 346, loss = 0.00097296\n",
      "Iteration 347, loss = 0.00117040\n",
      "Iteration 348, loss = 0.00098520\n",
      "Iteration 349, loss = 0.00108542\n",
      "Iteration 350, loss = 0.00122128\n",
      "Iteration 351, loss = 0.00122491\n",
      "Iteration 352, loss = 0.00133238\n",
      "Iteration 353, loss = 0.00115373\n",
      "Iteration 354, loss = 0.00113984\n",
      "Iteration 355, loss = 0.00100183\n",
      "Iteration 356, loss = 0.00095875\n",
      "Iteration 357, loss = 0.00095799\n",
      "Iteration 358, loss = 0.00094112\n",
      "Iteration 359, loss = 0.00096162\n",
      "Iteration 360, loss = 0.00095786\n",
      "Iteration 361, loss = 0.00092876\n",
      "Iteration 362, loss = 0.00095335\n",
      "Iteration 363, loss = 0.00091479\n",
      "Iteration 364, loss = 0.00092012\n",
      "Iteration 365, loss = 0.00113444\n",
      "Iteration 366, loss = 0.00091239\n",
      "Iteration 367, loss = 0.00095019\n",
      "Iteration 368, loss = 0.00094736\n",
      "Iteration 369, loss = 0.00089734\n",
      "Iteration 370, loss = 0.00086539\n",
      "Iteration 371, loss = 0.00095012\n",
      "Iteration 372, loss = 0.00088439\n",
      "Iteration 373, loss = 0.00093019\n",
      "Iteration 374, loss = 0.00089800\n",
      "Iteration 375, loss = 0.00083450\n",
      "Iteration 376, loss = 0.00087514\n",
      "Iteration 377, loss = 0.00083568\n",
      "Iteration 378, loss = 0.00084790\n",
      "Iteration 379, loss = 0.00091391\n",
      "Iteration 380, loss = 0.00085720\n",
      "Iteration 381, loss = 0.00082516\n",
      "Iteration 382, loss = 0.00082612\n",
      "Iteration 383, loss = 0.00083963\n",
      "Iteration 384, loss = 0.00087293\n",
      "Iteration 385, loss = 0.00081147\n",
      "Iteration 386, loss = 0.00084933\n",
      "Iteration 387, loss = 0.00081175\n",
      "Iteration 388, loss = 0.00079497\n",
      "Iteration 389, loss = 0.00084886\n",
      "Iteration 390, loss = 0.00080232\n",
      "Iteration 391, loss = 0.00080668\n",
      "Iteration 392, loss = 0.00083596\n",
      "Iteration 393, loss = 0.00075545\n",
      "Iteration 394, loss = 0.00080863\n",
      "Iteration 395, loss = 0.00089228\n",
      "Iteration 396, loss = 0.00076613\n",
      "Iteration 397, loss = 0.00085123\n",
      "Iteration 398, loss = 0.00078666\n",
      "Iteration 399, loss = 0.00076280\n",
      "Iteration 400, loss = 0.00075404\n",
      "Iteration 401, loss = 0.00075517\n",
      "Iteration 402, loss = 0.00072780\n",
      "Iteration 403, loss = 0.00071191\n",
      "Iteration 404, loss = 0.00081630\n",
      "Iteration 405, loss = 0.00072055\n",
      "Iteration 406, loss = 0.00075546\n",
      "Iteration 407, loss = 0.00075199\n",
      "Iteration 408, loss = 0.00074662\n",
      "Iteration 409, loss = 0.00073604\n",
      "Iteration 410, loss = 0.00077779\n",
      "Iteration 411, loss = 0.00086098\n",
      "Iteration 412, loss = 0.00076771\n",
      "Iteration 413, loss = 0.00074350\n",
      "Iteration 414, loss = 0.00072338\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100, 100), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=0, shuffle=True, solver='adam',\n",
       "              tol=1e-06, validation_fraction=0.1, verbose=True,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fazendo o treinamento com a base de dados de treinamento\n",
    "classificador.fit(previsores_treinamento, classe_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fazendo a predição usando a base de teste \n",
    "previsoes = classificador.predict(previsores_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0\n",
      " 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# observando os dados previstos com a base de teste\n",
    "print(previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0\n",
      " 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0\n",
      " 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# observando os resultados originais para a base de teste\n",
    "print(classe_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisando a capacidade de predição do algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando a biblioteca sklearn do python\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtendo a precisão do algoritmo\n",
    "precisao = accuracy_score(previsoes, classe_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.994\n"
     ]
    }
   ],
   "source": [
    "# observando a precisão de acertos do algoritmo\n",
    "print(precisao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A precisão de acertos do algoritmo foi de **99.4%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtendo a matriz de confusão que relaciona os acertos e erros do algoritmo\n",
    "matriz = confusion_matrix(previsoes, classe_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[435   2]\n",
      " [  1  62]]\n"
     ]
    }
   ],
   "source": [
    "# observando a matriz de confusão para os dados previstos pelo algoritmo\n",
    "print(matriz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que 435 dados referentes a pessoas que não possuem chances de pagar o que devem foram classificados corretamente, juntamente com 62 dados referentes a pessoas que possuem chance de pagar o que devem. Entretanto, 2 dados referentes a pessoas que não tem chance de pagar foram classificados de forma errada como pagadores, assim como 1 pessoa que tem chance de pagar foi classificada de forma errada como não pagador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observando o balaceamento das classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando a biblioteca collections do python\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 64, 0: 436})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualizando a quantidade de registros para cada uma das classes\n",
    "collections.Counter(classe_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alguma dúvida? Entre em contato comigo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Me envie um e-mail](mailto:alysson.barbosa@ee.ufcg.edu.br);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
