{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Técnicas de Ajustes de Pesos\n",
    "#### Entenda as principais técnicas usadas nas redes neurais para ajustar novos pesos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anteriormente, foi explicado que uma rede neural obtém conhecimento a partir da descoberta de pesos adequados para uma determinada base de dados. Existem diversas técnicas que podem ser utilizadas para fazer o ajustamento desses pesos, nesse arquivo será possível entender as principais técnicas usadas na área de inteligência artificial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método do Gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo desse método é tentar encontrar o menor erro possível, baseado nos pesos encontrados, para o algoritmo. Graficamente, existe uma curva que possui diversos **pontos mínimos**, mas o objetivo desse método é tentar encaminhar o valor dos pesos, de forma que o erro encontre um ponto de **mínimo global**, assim como está mostrado na imagem abaixo. A coloração mais avermelhada indica pontos em que o erro é alto e os pesos estão totalmente desajustados com a base de dados, e o ponto mais azulado é referente a pontos em que o erro é baixo e os pesos estão mais moldados com a base de dados utilizada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![descida-gradiente](Imagens/descida-gradiente.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matematicamente, temos a seguinte representação:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $min_c(w_{1}, w_{2}, ..., w_{n})$, sendo c a função de custo (**cost function**);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, o objetivo do método gradiente (**Gradient Descent**) é encontrar a melhor combinação de pesos em que o erro é o menor possível."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para isso, é necessário calcular o declive da curva (em que os pesos são os parâmetros) utilizando a ferramenta de **derivadas parciais**, visando dar a direção correta que os pesos devem seguir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além disso, o **cálculo do delta** é responsável por guiar o erro para que ele não fique preso a um mínimo local, fazendo um reajuste adequado dos pesos (vale a pena salientar que o delta será uma variável). O delta faz o ajuste tando na camada de saída quanto nas camadas ocultas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo é possível conferir um esquema completo dos passos feitos quando se utiliza o método do gradiente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![metodo-gradiente](Imagens/metodo-gradiente.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "superfície convexa = só há um mínimo global, não há mínimos locais\n",
    "(batch gradient descent) = calcula o erro para todos os registros e atualiza os pesos\n",
    "\n",
    "superfície não convexa = há mínimos locais e global\n",
    "(stochastic gradient descent) = calcula o erro para cada registro e atualiza os pesos\n",
    "- ajuda a previnir mínimos locais (superfícies não convexas)\n",
    "- mais rápido (não precisa carregar todos os dados em memória)\n",
    "\n",
    "Mini batch gradient descent = escolhe um número de registros para rodar e atualizar os pesos (parâmetro bath_size configurável)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ter os seguintes modelos para o método do gradiente, depedendo do problema a ser resolvido:\n",
    "- **Superfícies convexas** = só há um mínimo global, não há mínimos locais. Para esse problema usa-se o método **batch gradient descent**, que consiste em calcular o erro para todos os registros para atualizar os pesos.\n",
    "- **Superfícies não convexas** = há mínimos locais e globais. Para esse problema usa-se o método **stochastic gradient descent**, que consiste em calcular o erro para cada registro e atuaiza os pesos. Assim, é possível previnir mínimos locais e a execução do treinamento da rede neural é mais rápida, pois não será preciso carregar todos os dados na memória para atualizá-los. Existe também o melhoramento **mini batch gradient descent**, que consiste em escolher um número de registros para rodar e atualizar os pesos (parâmetro batch_size configurável)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O backpropagation é um método que atualiza os pesos conforme os resultados obtidos na execução anterior, é uma meneira de criar um feedback  sobre a entrada e a saída."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O cálculo dos novos pesos pelo método backpropagation é o seguinte:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $peso_{n+1} = (peso_{n} * momento) + (entrada * delta * TaxaAprendizagem)$;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe o significado de alguns parâmetros utilizados:  \n",
    "- **Taxa de Aprendizagem** (**Learning Rate**): define o quão rápido o algoritmo vai aprender. Um valor alto indica que os pesos irão convergir mais rapidamente, porém o erro pode perder o mínimo global. Um valor baixo será mais lento, porém tem mais chances do erro chegar a um mínimo global.\n",
    "- **Momento** (**Momentum**): Escapar de mínimos locais (nem sempre funciona). Define o qão confiável é a última alteração dos pesos. Um valor alto aumenta a velocidade da convergência com menos precisão de acerto e um valor baixo pode evitar mínimos locais com mais precisão de acertos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outros métodos famosos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Há outras formas também de se conseguir ajustar os pesos, como o método da **força bruta** (analisar todos os pesos possíveis), **simmulated anealing** e **algoritmos genéticos** (algoritmos de otimização para encontrar valores mínimos com as melhores escolhas dos pesos). Esses algoritmos de otimização correspondem a uma área de estudo da inteligência artificial, assim como o **método do gradiente** e o **backpropagation**, por esse motivo que os conceitos foram explicados de forma superficial, dado que há muita complexidade por trás dessas ideias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alguma dúvida? Entre em contato comigo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Me envie um e-mail](mailto:alysson.barbosa@ee.ufcg.edu.br);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
