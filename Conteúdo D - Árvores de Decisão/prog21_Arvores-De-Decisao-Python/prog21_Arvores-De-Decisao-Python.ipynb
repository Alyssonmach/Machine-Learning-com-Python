{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Árvores de Decisão\n",
    "### Algoritmo árvores de decisão para uma base de dados do censo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse arquivo, será feito uma predição para pessoas que possuem uma renda anual maior que 50 mil ou menor que 50 mil com base em uma série de dados para cada um dos usuários. Para esse fim, será usado uma base de dados do censo americano e o algoritmo pelo método de árvores de decisão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pré-processamento da base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando a biblioteca pandas do python\n",
    "import pandas as pd\n",
    "\n",
    "# atribuindo ao objeto \"dataframe\" todos os registros do banco de dados armazenados no arquivo \"census.csv\"\n",
    "dataframe = pd.read_csv(\"census.csv\", encoding = \"UTF-8\", sep = \",\")\n",
    "\n",
    "# separando os atributos previsores do \"dataframe\"\n",
    "previsores = dataframe.iloc[:,0:14].values\n",
    "\n",
    "# separando os atributos meta classe do \"dataframe\"\n",
    "meta_classe = dataframe.iloc[:, 14].values\n",
    "\n",
    "# importando a biblioteca sklearn do Python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# LabelEncoder é uma função responsável por  normalizar rótulos\n",
    "# o objetivo é transoformar variáveis categóricas em numéricas\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# importando a biblioteca sklearn do Python\n",
    "# função \"OneHotEncoder\" responsável por fazer a trasnformação em variáveis \"dummy\"\n",
    "\n",
    "# importando a biblioteca sklearn do Python\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# função \"ColumnTransformer\" responsável por definir quais colunas o objeto irá agir na alteração\n",
    "\n",
    "# criando o nosso objeto \"LabelEncoder_previsores\" para fazer a transformação dos campos de dados categóricos em numéricos\n",
    "LabelEncoder_previsores = LabelEncoder()\n",
    "\n",
    "# aplicando a transformação em todos os campos de dados categóricos usando o objeto criado e definido \"LabelEncoder_previsores\"\n",
    "previsores[:,1] = LabelEncoder_previsores.fit_transform(previsores[:,1])\n",
    "previsores[:,3] = LabelEncoder_previsores.fit_transform(previsores[:,3])\n",
    "previsores[:,5] = LabelEncoder_previsores.fit_transform(previsores[:,5])\n",
    "previsores[:,6] = LabelEncoder_previsores.fit_transform(previsores[:,6])\n",
    "previsores[:,7] = LabelEncoder_previsores.fit_transform(previsores[:,7])\n",
    "previsores[:,8] = LabelEncoder_previsores.fit_transform(previsores[:,8])\n",
    "previsores[:,9] = LabelEncoder_previsores.fit_transform(previsores[:,9])\n",
    "previsores[:,13] = LabelEncoder_previsores.fit_transform(previsores[:,13])\n",
    "\n",
    "# criando e configurando o objeto \"onehotencoder\" \n",
    "onehotencoder = ColumnTransformer(transformers = [(\"OneHot\", OneHotEncoder(), [1,3,5,6,7,8,9,13])], remainder = \"passthrough\")\n",
    "\n",
    "# transformando os atributos previsores em variáveis dummy usando o objeto \"onehotencoder\"\n",
    "previsores = onehotencoder.fit_transform(previsores).toarray()\n",
    "\n",
    "# importando a biblioteca sklearn do Python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# função \"StandardScaler\" responsável por fazer escalonamento de atributos\n",
    "\n",
    "# criando e configurando o objeto \"scaler\"\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# aplicando o escalonamento nos atributos previsores\n",
    "previsores = scaler.fit_transform(previsores)\n",
    "\n",
    "# importando a biblioteca sklearn do Python\n",
    "from sklearn.model_selection import train_test_split\n",
    "# a função \"train_test_split\" tem a função de separar modelos de treinamento e modelos de teste em uma base de dados\n",
    "\n",
    "# definir os modelos de treinamento e modelos de teste para os atributos previsores e meta_classe\n",
    "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, meta_classe, \n",
    "                                                                                              test_size = 0.15,\n",
    "                                                                                             random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algoritmo de árvores de decisão com python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando o pacote sklearn do python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# função \"DecisionTreeClassifier\" responsável por criar a árvore de decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando o objeto \"classificador\", esse que receberá a árvore de decisão relativa a base de dados trabalhada\n",
    "classificador = DecisionTreeClassifier(criterion = \"entropy\", random_state = 0)\n",
    "# usa a fórmula da entropia e do ganho para fazer o critério dos dados mais relevantes\n",
    "# definine um estado randômico como zero para garantir que em qualquer ocasião o algoritmo obtenha o mesmo resultado para uma\n",
    "# base de dados analisada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=0, splitter='best')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fazendo o treinamento com a base de dados \n",
    "classificador.fit(previsores_treinamento, classe_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usando a base de dados de teste para analisar a capacidade de predição do algoritmo feito\n",
    "previsoes = classificador.predict(previsores_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' <=50K' ' <=50K' ' <=50K' ... ' <=50K' ' <=50K' ' <=50K']\n"
     ]
    }
   ],
   "source": [
    "# observando a classe teste\n",
    "print(classe_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' <=50K' ' <=50K' ' <=50K' ... ' <=50K' ' <=50K' ' >50K']\n"
     ]
    }
   ],
   "source": [
    "# observando o resultado do nosso algoritmo, esse que deve ser semelhante a \"classe_teste\" para um resultado ideal\n",
    "print(previsoes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparando a base de dados de teste com o resultado do algoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acima podem ser visualizadas duas bases de dados: a primeira relativa aos dados confiáveis e a segunda relativa ao resultado do algoritmo. Lembrando que:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**Saída de Dados (Legenda)**|\n",
    "|:---------------------------|\n",
    "|\"<=50K\" = renda menor que 50 mil|\n",
    "|\">50K\" = renda maior que 50 mil|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando o pacote sklearn do python\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "# \"confusion_matrix\" relaciona os dados originais com os dados previstos pelo algoritmo em uma matriz\n",
    "# \"accuracy_score\" exibe uma porcentagem de confiança no algoritmo, com base nos dados originais e nos dados previstos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usando \"accuracy_score\" para obter a precisão do algoritmo\n",
    "precisao = accuracy_score(classe_teste, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8104401228249745\n"
     ]
    }
   ],
   "source": [
    "# exibindo na saída de dados a porcentagem de acertos obtidos pelo algoritmo feito\n",
    "print(precisao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizando todo o pré-processamento (transformando variáveis categóricas em numéricas, transformando as variáveis em dummy e escalonando-a) obtivemos uma porcentagem de acerto de **81.04%**. Agora, convém mudar alguns pré-processamentos feitos para tentar obter um resultado melhor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**Resultados do algoritmo**|\n",
    "|:--------------------------|\n",
    "|(labelencoder + onehotencoder + escalonamento) = **81.04%**|\n",
    "|(labelencoder + escalonamento) = |\n",
    "|(labelencoder + onehotencoder) = |\n",
    "|(labelencoder) = |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando a biblioteca pandas do python\n",
    "import pandas as pd\n",
    "\n",
    "# atribuindo ao objeto \"dataframe\" todos os registros do banco de dados armazenados no arquivo \"census.csv\"\n",
    "dataframe = pd.read_csv(\"census.csv\", encoding = \"UTF-8\", sep = \",\")\n",
    "\n",
    "# separando os atributos previsores do \"dataframe\"\n",
    "previsores = dataframe.iloc[:,0:14].values\n",
    "\n",
    "# separando os atributos meta classe do \"dataframe\"\n",
    "meta_classe = dataframe.iloc[:, 14].values\n",
    "\n",
    "# importando a biblioteca sklearn do Python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# LabelEncoder é uma função responsável por  normalizar rótulos\n",
    "# o objetivo é transoformar variáveis categóricas em numéricas\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# importando a biblioteca sklearn do Python\n",
    "# função \"OneHotEncoder\" responsável por fazer a trasnformação em variáveis \"dummy\"\n",
    "\n",
    "# importando a biblioteca sklearn do Python\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# função \"ColumnTransformer\" responsável por definir quais colunas o objeto irá agir na alteração\n",
    "\n",
    "# criando o nosso objeto \"LabelEncoder_previsores\" para fazer a transformação dos campos de dados categóricos em numéricos\n",
    "LabelEncoder_previsores = LabelEncoder()\n",
    "\n",
    "# aplicando a transformação em todos os campos de dados categóricos usando o objeto criado e definido \"LabelEncoder_previsores\"\n",
    "previsores[:,1] = LabelEncoder_previsores.fit_transform(previsores[:,1])\n",
    "previsores[:,3] = LabelEncoder_previsores.fit_transform(previsores[:,3])\n",
    "previsores[:,5] = LabelEncoder_previsores.fit_transform(previsores[:,5])\n",
    "previsores[:,6] = LabelEncoder_previsores.fit_transform(previsores[:,6])\n",
    "previsores[:,7] = LabelEncoder_previsores.fit_transform(previsores[:,7])\n",
    "previsores[:,8] = LabelEncoder_previsores.fit_transform(previsores[:,8])\n",
    "previsores[:,9] = LabelEncoder_previsores.fit_transform(previsores[:,9])\n",
    "previsores[:,13] = LabelEncoder_previsores.fit_transform(previsores[:,13])\n",
    "\n",
    "# criando e configurando o objeto \"onehotencoder\" \n",
    "#onehotencoder = ColumnTransformer(transformers = [(\"OneHot\", OneHotEncoder(), [1,3,5,6,7,8,9,13])], remainder = \"passthrough\")\n",
    "\n",
    "# transformando os atributos previsores em variáveis dummy usando o objeto \"onehotencoder\"\n",
    "#previsores = onehotencoder.fit_transform(previsores).toarray()\n",
    "\n",
    "# importando a biblioteca sklearn do Python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# função \"StandardScaler\" responsável por fazer escalonamento de atributos\n",
    "\n",
    "# criando e configurando o objeto \"scaler\"\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# aplicando o escalonamento nos atributos previsores\n",
    "previsores = scaler.fit_transform(previsores)\n",
    "\n",
    "## importando a biblioteca sklearn do Python\n",
    "from sklearn.model_selection import train_test_split\n",
    "# a função \"train_test_split\" tem a função de separar modelos de treinamento e modelos de teste em uma base de dados\n",
    "\n",
    "# definir os modelos de treinamento e modelos de teste para os atributos previsores e meta_classe\n",
    "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, meta_classe, \n",
    "                                                                                              test_size = 0.15,\n",
    "                                                                                             random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8128966223132037\n"
     ]
    }
   ],
   "source": [
    "# importando o pacote sklearn do python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# função \"DecisionTreeClassifier\" responsável por criar a árvore de decisão\n",
    "\n",
    "# criando o objeto \"classificador\", esse que receberá a árvore de decisão relativa a base de dados trabalhada\n",
    "classificador = DecisionTreeClassifier(criterion = \"entropy\", random_state = 0)\n",
    "# usa a fórmula da entropia e do ganho para fazer o critério dos dados mais relevantes\n",
    "# definine um estado randômico como zero para garantir que em qualquer ocasião o algoritmo obtenha o mesmo resultado para uma\n",
    "# base de dados analisada\n",
    "\n",
    "# fazendo o treinamento com a base de dados \n",
    "classificador.fit(previsores_treinamento, classe_treinamento)\n",
    "\n",
    "# usando a base de dados de teste para analisar a capacidade de predição do algoritmo feito\n",
    "previsoes = classificador.predict(previsores_teste)\n",
    "\n",
    "# importando o pacote sklearn do python\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "# \"confusion_matrix\" relaciona os dados originais com os dados previstos pelo algoritmo em uma matriz\n",
    "# \"accuracy_score\" exibe uma porcentagem de confiança no algoritmo, com base nos dados originais e nos dados previstos\n",
    "\n",
    "# usando \"accuracy_score\" para obter a precisão do algoritmo\n",
    "precisao = accuracy_score(classe_teste, previsoes)\n",
    "\n",
    "# exibindo na saída de dados a porcentagem de acertos obtidos pelo algoritmo feito\n",
    "print(precisao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**Resultados do algoritmo**|\n",
    "|:--------------------------|\n",
    "|(labelencoder + onehotencoder + escalonamento) = **81.04%**|\n",
    "|(labelencoder + escalonamento) = **81.29%**|\n",
    "|(labelencoder + onehotencoder) = |\n",
    "|(labelencoder) = |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando a biblioteca pandas do python\n",
    "import pandas as pd\n",
    "\n",
    "# atribuindo ao objeto \"dataframe\" todos os registros do banco de dados armazenados no arquivo \"census.csv\"\n",
    "dataframe = pd.read_csv(\"census.csv\", encoding = \"UTF-8\", sep = \",\")\n",
    "\n",
    "# separando os atributos previsores do \"dataframe\"\n",
    "previsores = dataframe.iloc[:,0:14].values\n",
    "\n",
    "# separando os atributos meta classe do \"dataframe\"\n",
    "meta_classe = dataframe.iloc[:, 14].values\n",
    "\n",
    "# importando a biblioteca sklearn do Python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# LabelEncoder é uma função responsável por  normalizar rótulos\n",
    "# o objetivo é transoformar variáveis categóricas em numéricas\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# importando a biblioteca sklearn do Python\n",
    "# função \"OneHotEncoder\" responsável por fazer a trasnformação em variáveis \"dummy\"\n",
    "\n",
    "# importando a biblioteca sklearn do Python\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# função \"ColumnTransformer\" responsável por definir quais colunas o objeto irá agir na alteração\n",
    "\n",
    "# criando o nosso objeto \"LabelEncoder_previsores\" para fazer a transformação dos campos de dados categóricos em numéricos\n",
    "LabelEncoder_previsores = LabelEncoder()\n",
    "\n",
    "# aplicando a transformação em todos os campos de dados categóricos usando o objeto criado e definido \"LabelEncoder_previsores\"\n",
    "previsores[:,1] = LabelEncoder_previsores.fit_transform(previsores[:,1])\n",
    "previsores[:,3] = LabelEncoder_previsores.fit_transform(previsores[:,3])\n",
    "previsores[:,5] = LabelEncoder_previsores.fit_transform(previsores[:,5])\n",
    "previsores[:,6] = LabelEncoder_previsores.fit_transform(previsores[:,6])\n",
    "previsores[:,7] = LabelEncoder_previsores.fit_transform(previsores[:,7])\n",
    "previsores[:,8] = LabelEncoder_previsores.fit_transform(previsores[:,8])\n",
    "previsores[:,9] = LabelEncoder_previsores.fit_transform(previsores[:,9])\n",
    "previsores[:,13] = LabelEncoder_previsores.fit_transform(previsores[:,13])\n",
    "\n",
    "# criando e configurando o objeto \"onehotencoder\" \n",
    "onehotencoder = ColumnTransformer(transformers = [(\"OneHot\", OneHotEncoder(), [1,3,5,6,7,8,9,13])], remainder = \"passthrough\")\n",
    "\n",
    "# transformando os atributos previsores em variáveis dummy usando o objeto \"onehotencoder\"\n",
    "previsores = onehotencoder.fit_transform(previsores).toarray()\n",
    "\n",
    "# importando a biblioteca sklearn do Python\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "# função \"StandardScaler\" responsável por fazer escalonamento de atributos\n",
    "\n",
    "# criando e configurando o objeto \"scaler\"\n",
    "#scaler = StandardScaler()\n",
    "\n",
    "# aplicando o escalonamento nos atributos previsores\n",
    "#previsores = scaler.fit_transform(previsores)\n",
    "\n",
    "## importando a biblioteca sklearn do Python\n",
    "from sklearn.model_selection import train_test_split\n",
    "# a função \"train_test_split\" tem a função de separar modelos de treinamento e modelos de teste em uma base de dados\n",
    "\n",
    "# definir os modelos de treinamento e modelos de teste para os atributos previsores e meta_classe\n",
    "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, meta_classe, \n",
    "                                                                                              test_size = 0.15,\n",
    "                                                                                             random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8102354145342886\n"
     ]
    }
   ],
   "source": [
    "# importando o pacote sklearn do python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# função \"DecisionTreeClassifier\" responsável por criar a árvore de decisão\n",
    "\n",
    "# criando o objeto \"classificador\", esse que receberá a árvore de decisão relativa a base de dados trabalhada\n",
    "classificador = DecisionTreeClassifier(criterion = \"entropy\", random_state = 0)\n",
    "# usa a fórmula da entropia e do ganho para fazer o critério dos dados mais relevantes\n",
    "# definine um estado randômico como zero para garantir que em qualquer ocasião o algoritmo obtenha o mesmo resultado para uma\n",
    "# base de dados analisada\n",
    "\n",
    "# fazendo o treinamento com a base de dados \n",
    "classificador.fit(previsores_treinamento, classe_treinamento)\n",
    "\n",
    "# usando a base de dados de teste para analisar a capacidade de predição do algoritmo feito\n",
    "previsoes = classificador.predict(previsores_teste)\n",
    "\n",
    "# importando o pacote sklearn do python\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "# \"confusion_matrix\" relaciona os dados originais com os dados previstos pelo algoritmo em uma matriz\n",
    "# \"accuracy_score\" exibe uma porcentagem de confiança no algoritmo, com base nos dados originais e nos dados previstos\n",
    "\n",
    "# usando \"accuracy_score\" para obter a precisão do algoritmo\n",
    "precisao = accuracy_score(classe_teste, previsoes)\n",
    "\n",
    "# exibindo na saída de dados a porcentagem de acertos obtidos pelo algoritmo feito\n",
    "print(precisao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**Resultados do algoritmo**|\n",
    "|:--------------------------|\n",
    "|(labelencoder + onehotencoder + escalonamento) = **81.04%**|\n",
    "|(labelencoder + escalonamento) = **81.29%**|\n",
    "|(labelencoder + onehotencoder) = **81.02%**|\n",
    "|(labelencoder) = |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando a biblioteca pandas do python\n",
    "import pandas as pd\n",
    "\n",
    "# atribuindo ao objeto \"dataframe\" todos os registros do banco de dados armazenados no arquivo \"census.csv\"\n",
    "dataframe = pd.read_csv(\"census.csv\", encoding = \"UTF-8\", sep = \",\")\n",
    "\n",
    "# separando os atributos previsores do \"dataframe\"\n",
    "previsores = dataframe.iloc[:,0:14].values\n",
    "\n",
    "# separando os atributos meta classe do \"dataframe\"\n",
    "meta_classe = dataframe.iloc[:, 14].values\n",
    "\n",
    "# importando a biblioteca sklearn do Python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# LabelEncoder é uma função responsável por  normalizar rótulos\n",
    "# o objetivo é transoformar variáveis categóricas em numéricas\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# importando a biblioteca sklearn do Python\n",
    "# função \"OneHotEncoder\" responsável por fazer a trasnformação em variáveis \"dummy\"\n",
    "\n",
    "# importando a biblioteca sklearn do Python\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# função \"ColumnTransformer\" responsável por definir quais colunas o objeto irá agir na alteração\n",
    "\n",
    "# criando o nosso objeto \"LabelEncoder_previsores\" para fazer a transformação dos campos de dados categóricos em numéricos\n",
    "LabelEncoder_previsores = LabelEncoder()\n",
    "\n",
    "# aplicando a transformação em todos os campos de dados categóricos usando o objeto criado e definido \"LabelEncoder_previsores\"\n",
    "previsores[:,1] = LabelEncoder_previsores.fit_transform(previsores[:,1])\n",
    "previsores[:,3] = LabelEncoder_previsores.fit_transform(previsores[:,3])\n",
    "previsores[:,5] = LabelEncoder_previsores.fit_transform(previsores[:,5])\n",
    "previsores[:,6] = LabelEncoder_previsores.fit_transform(previsores[:,6])\n",
    "previsores[:,7] = LabelEncoder_previsores.fit_transform(previsores[:,7])\n",
    "previsores[:,8] = LabelEncoder_previsores.fit_transform(previsores[:,8])\n",
    "previsores[:,9] = LabelEncoder_previsores.fit_transform(previsores[:,9])\n",
    "previsores[:,13] = LabelEncoder_previsores.fit_transform(previsores[:,13])\n",
    "\n",
    "# criando e configurando o objeto \"onehotencoder\" \n",
    "#onehotencoder = ColumnTransformer(transformers = [(\"OneHot\", OneHotEncoder(), [1,3,5,6,7,8,9,13])], remainder = \"passthrough\")\n",
    "\n",
    "# transformando os atributos previsores em variáveis dummy usando o objeto \"onehotencoder\"\n",
    "#previsores = onehotencoder.fit_transform(previsores).toarray()\n",
    "\n",
    "# importando a biblioteca sklearn do Python\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "# função \"StandardScaler\" responsável por fazer escalonamento de atributos\n",
    "\n",
    "# criando e configurando o objeto \"scaler\"\n",
    "#scaler = StandardScaler()\n",
    "\n",
    "# aplicando o escalonamento nos atributos previsores\n",
    "#previsores = scaler.fit_transform(previsores)\n",
    "\n",
    "## importando a biblioteca sklearn do Python\n",
    "from sklearn.model_selection import train_test_split\n",
    "# a função \"train_test_split\" tem a função de separar modelos de treinamento e modelos de teste em uma base de dados\n",
    "\n",
    "# definir os modelos de treinamento e modelos de teste para os atributos previsores e meta_classe\n",
    "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, meta_classe, \n",
    "                                                                                              test_size = 0.15,\n",
    "                                                                                             random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8128966223132037\n"
     ]
    }
   ],
   "source": [
    "# importando o pacote sklearn do python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# função \"DecisionTreeClassifier\" responsável por criar a árvore de decisão\n",
    "\n",
    "# criando o objeto \"classificador\", esse que receberá a árvore de decisão relativa a base de dados trabalhada\n",
    "classificador = DecisionTreeClassifier(criterion = \"entropy\", random_state = 0)\n",
    "# usa a fórmula da entropia e do ganho para fazer o critério dos dados mais relevantes\n",
    "# definine um estado randômico como zero para garantir que em qualquer ocasião o algoritmo obtenha o mesmo resultado para uma\n",
    "# base de dados analisada\n",
    "\n",
    "# fazendo o treinamento com a base de dados \n",
    "classificador.fit(previsores_treinamento, classe_treinamento)\n",
    "\n",
    "# usando a base de dados de teste para analisar a capacidade de predição do algoritmo feito\n",
    "previsoes = classificador.predict(previsores_teste)\n",
    "\n",
    "# importando o pacote sklearn do python\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "# \"confusion_matrix\" relaciona os dados originais com os dados previstos pelo algoritmo em uma matriz\n",
    "# \"accuracy_score\" exibe uma porcentagem de confiança no algoritmo, com base nos dados originais e nos dados previstos\n",
    "\n",
    "# usando \"accuracy_score\" para obter a precisão do algoritmo\n",
    "precisao = accuracy_score(classe_teste, previsoes)\n",
    "\n",
    "# exibindo na saída de dados a porcentagem de acertos obtidos pelo algoritmo feito\n",
    "print(precisao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**Resultados do algoritmo**|\n",
    "|:--------------------------|\n",
    "|(labelencoder + onehotencoder + escalonamento) = **81.04%**|\n",
    "|(labelencoder + escalonamento) = **81.29%**|\n",
    "|(labelencoder + onehotencoder) = **81.02%**|\n",
    "|(labelencoder) = **81.29%**|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como resultado, é possível observar que fazer a transformação das variáveis categórias é pré-requisito para obter um bom resultado. Entretanto, transformar as variáveis em tipo dummy não é tão bom assim e o escalonamento de atributos é opcional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O melhor resultado do algoritmo por **árvores de decisão** foi **81.29%**, comparando com o resultado do algoritmo com **naive bayes** há o seguinte resultado:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**Comparando os algoritmos com a mesma base de dados para classificação**:|\n",
    "|:----------|\n",
    "|Naive Bayes (Porcentagem de acertos): **80,57%%**|\n",
    "|Árvores de Decisão (Porcentagem de acertos): **81.29%**|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concluí-se que o algoritmo usando o método de árvores de decisão foi mais favorável para resolver esse problema de classificação censitária."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alguma dúvida? Entre em contato comigo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Me envie um e-mail](mailto:alysson.barbosa@ee.ufcg.edu.br);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
