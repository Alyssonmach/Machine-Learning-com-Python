{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rede Neural para os Dados do Censo\n",
    "#### Criando uma rede neural para resolver o problema da base do censo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando a biblioteca pandas do python\n",
    "import pandas as pd\n",
    "\n",
    "# atribuindo os registros da base de dados para o objeto \"dataframe\"\n",
    "dataframe = pd.read_csv('census.csv', encoding = 'utf-8', sep = ',')\n",
    "\n",
    "# separando os atributos previsores e classe \n",
    "previsores = dataframe.iloc[:, 0:14].values\n",
    "classe = dataframe.iloc[:, 14].values\n",
    "\n",
    "# importando a biblioteca sklearn do python\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "# LabelEncoder é uma função responsável por  normalizar rótulos\n",
    "# o objetivo é transoformar variáveis categóricas em numéricas\n",
    "# função \"OneHotEncoder\" responsável por fazer a trasnformação em variáveis \"dummy\"\n",
    "\n",
    "# importando a biblioteca sklearn do python\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# função \"ColumnTransformer\" responsável por definir quais colunas o objeto irá agir na alteração\n",
    "\n",
    "# criando o objeto \"labelencoder_previsores\" para fazer a transformação dos atributos categóricos em numéricos\n",
    "labelencoder_previsores = LabelEncoder()\n",
    "\n",
    "# aplicando a transformação em todos os campos de dados categóricos usando o objeto criado e definido \n",
    "# \"LabelEncoder_previsores\"\n",
    "previsores[:,1] = labelencoder_previsores.fit_transform(previsores[:,1])\n",
    "previsores[:,3] = labelencoder_previsores.fit_transform(previsores[:,3])\n",
    "previsores[:,5] = labelencoder_previsores.fit_transform(previsores[:,5])\n",
    "previsores[:,6] = labelencoder_previsores.fit_transform(previsores[:,6])\n",
    "previsores[:,7] = labelencoder_previsores.fit_transform(previsores[:,7])\n",
    "previsores[:,8] = labelencoder_previsores.fit_transform(previsores[:,8])\n",
    "previsores[:,9] = labelencoder_previsores.fit_transform(previsores[:,9])\n",
    "previsores[:,13] = labelencoder_previsores.fit_transform(previsores[:,13])\n",
    "\n",
    "# criando e configurando o objeto onehotencoder\n",
    "onehotencoder = ColumnTransformer(transformers = [('OneHot', OneHotEncoder(), [1,3,5,6,7,8,9,13])], \n",
    "                                  remainder = 'passthrough')\n",
    "\n",
    "# transformando os atributos previsores em variáveis do tipo \"dummy\"\n",
    "previsores = onehotencoder.fit_transform(previsores).toarray()\n",
    "\n",
    "# importando a biblioteca sklearn do python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# criando e configurando o objeto \"scaler\"\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# aplicando o escaolonamento nos atributos previsores\n",
    "previsores = scaler.fit_transform(previsores)\n",
    "\n",
    "# importando a biblioteca sklearn do python\n",
    "from sklearn.model_selection import train_test_split\n",
    "# a função \"train_test_split\" tem a função de separar modelos de treinamento e modelos de teste em uma base \n",
    "#de dados\n",
    "\n",
    "# separando as bases de dados entre modelos de treinamento e modelos de teste\n",
    "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, \n",
    "                                                                                             classe,\n",
    "                                                                                             test_size = 0.15, \n",
    "                                                                                             random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando uma rede neural usando a linguagem de programação Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pré-processamento: **LabelEncoder + OneHotEncoder + StandardScaler**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando a biblioteca sklearn do python\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# 'MLPClassifier' é usado para criar uma rede neural\n",
    "# Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando e definindo um 'classificador' para definir os parâmetros da rede neural\n",
    "# verbose mostra na tela os dados em cada época de treinamento\n",
    "# max_iter = números de repetições, ou seja, quantidade de vezes que o algoritmo será treinado\n",
    "# tol = ajusta os pesos com passos pequenos\n",
    "# solver é a otimização utilizada, 'adam' é um melhoramente do método gradiente descente\n",
    "# activation = função de ativação\n",
    "# hidden_layer_sizes = quantidade de neurônios em cada camada oculta\n",
    "# learning_rate = indica o método de ajuste de pesos, em 'constant' o treinamento para quando o peso não \n",
    "# mude por duas vezes consecutivas e 'adaptative' muda a taxa de aprendizado quando o erro não diminui\n",
    "# muda a taxa de aprendizagem\n",
    "classificador = MLPClassifier(verbose = True, max_iter = 1000, tol = 0.0000010, solver = 'adam', \n",
    "                              hidden_layer_sizes = (100, 100), activation = 'relu', \n",
    "                              learning_rate = 'constant', random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.37925477\n",
      "Iteration 2, loss = 0.32066442\n",
      "Iteration 3, loss = 0.30993984\n",
      "Iteration 4, loss = 0.30350296\n",
      "Iteration 5, loss = 0.29803262\n",
      "Iteration 6, loss = 0.29346924\n",
      "Iteration 7, loss = 0.29025075\n",
      "Iteration 8, loss = 0.28646792\n",
      "Iteration 9, loss = 0.28434745\n",
      "Iteration 10, loss = 0.28062483\n",
      "Iteration 11, loss = 0.27877842\n",
      "Iteration 12, loss = 0.27506406\n",
      "Iteration 13, loss = 0.27344820\n",
      "Iteration 14, loss = 0.27004223\n",
      "Iteration 15, loss = 0.26764467\n",
      "Iteration 16, loss = 0.26573897\n",
      "Iteration 17, loss = 0.26228694\n",
      "Iteration 18, loss = 0.25961221\n",
      "Iteration 19, loss = 0.25741437\n",
      "Iteration 20, loss = 0.25483812\n",
      "Iteration 21, loss = 0.25333541\n",
      "Iteration 22, loss = 0.25136399\n",
      "Iteration 23, loss = 0.24761167\n",
      "Iteration 24, loss = 0.24634396\n",
      "Iteration 25, loss = 0.24236574\n",
      "Iteration 26, loss = 0.23986850\n",
      "Iteration 27, loss = 0.24033890\n",
      "Iteration 28, loss = 0.23611067\n",
      "Iteration 29, loss = 0.23542471\n",
      "Iteration 30, loss = 0.23121316\n",
      "Iteration 31, loss = 0.23130414\n",
      "Iteration 32, loss = 0.22947344\n",
      "Iteration 33, loss = 0.22713004\n",
      "Iteration 34, loss = 0.22443281\n",
      "Iteration 35, loss = 0.22307210\n",
      "Iteration 36, loss = 0.22059353\n",
      "Iteration 37, loss = 0.21867964\n",
      "Iteration 38, loss = 0.21664170\n",
      "Iteration 39, loss = 0.21528471\n",
      "Iteration 40, loss = 0.21405105\n",
      "Iteration 41, loss = 0.21320039\n",
      "Iteration 42, loss = 0.21113452\n",
      "Iteration 43, loss = 0.20704640\n",
      "Iteration 44, loss = 0.20690030\n",
      "Iteration 45, loss = 0.20453192\n",
      "Iteration 46, loss = 0.20385012\n",
      "Iteration 47, loss = 0.20164639\n",
      "Iteration 48, loss = 0.20125522\n",
      "Iteration 49, loss = 0.19895418\n",
      "Iteration 50, loss = 0.19708479\n",
      "Iteration 51, loss = 0.19588168\n",
      "Iteration 52, loss = 0.19494231\n",
      "Iteration 53, loss = 0.19716086\n",
      "Iteration 54, loss = 0.19193442\n",
      "Iteration 55, loss = 0.19110599\n",
      "Iteration 56, loss = 0.19109573\n",
      "Iteration 57, loss = 0.19013941\n",
      "Iteration 58, loss = 0.18800375\n",
      "Iteration 59, loss = 0.18638583\n",
      "Iteration 60, loss = 0.18529101\n",
      "Iteration 61, loss = 0.18443768\n",
      "Iteration 62, loss = 0.18344956\n",
      "Iteration 63, loss = 0.17997725\n",
      "Iteration 64, loss = 0.18005837\n",
      "Iteration 65, loss = 0.17945397\n",
      "Iteration 66, loss = 0.17835171\n",
      "Iteration 67, loss = 0.17712185\n",
      "Iteration 68, loss = 0.17801996\n",
      "Iteration 69, loss = 0.17554862\n",
      "Iteration 70, loss = 0.17327054\n",
      "Iteration 71, loss = 0.17447209\n",
      "Iteration 72, loss = 0.17410145\n",
      "Iteration 73, loss = 0.17143426\n",
      "Iteration 74, loss = 0.17200406\n",
      "Iteration 75, loss = 0.17008141\n",
      "Iteration 76, loss = 0.17092382\n",
      "Iteration 77, loss = 0.16823594\n",
      "Iteration 78, loss = 0.16761103\n",
      "Iteration 79, loss = 0.16700093\n",
      "Iteration 80, loss = 0.16697416\n",
      "Iteration 81, loss = 0.16322980\n",
      "Iteration 82, loss = 0.16318888\n",
      "Iteration 83, loss = 0.16264453\n",
      "Iteration 84, loss = 0.16235504\n",
      "Iteration 85, loss = 0.16188412\n",
      "Iteration 86, loss = 0.15987393\n",
      "Iteration 87, loss = 0.15922793\n",
      "Iteration 88, loss = 0.15953049\n",
      "Iteration 89, loss = 0.15735935\n",
      "Iteration 90, loss = 0.15909568\n",
      "Iteration 91, loss = 0.15794056\n",
      "Iteration 92, loss = 0.15675778\n",
      "Iteration 93, loss = 0.15720638\n",
      "Iteration 94, loss = 0.15487510\n",
      "Iteration 95, loss = 0.15389527\n",
      "Iteration 96, loss = 0.15257221\n",
      "Iteration 97, loss = 0.15294313\n",
      "Iteration 98, loss = 0.15213645\n",
      "Iteration 99, loss = 0.15104728\n",
      "Iteration 100, loss = 0.14903138\n",
      "Iteration 101, loss = 0.15042510\n",
      "Iteration 102, loss = 0.14762836\n",
      "Iteration 103, loss = 0.14889391\n",
      "Iteration 104, loss = 0.15085923\n",
      "Iteration 105, loss = 0.14879121\n",
      "Iteration 106, loss = 0.14727093\n",
      "Iteration 107, loss = 0.14683228\n",
      "Iteration 108, loss = 0.14572495\n",
      "Iteration 109, loss = 0.14656553\n",
      "Iteration 110, loss = 0.14794395\n",
      "Iteration 111, loss = 0.14567736\n",
      "Iteration 112, loss = 0.14500046\n",
      "Iteration 113, loss = 0.14443316\n",
      "Iteration 114, loss = 0.14167213\n",
      "Iteration 115, loss = 0.14170346\n",
      "Iteration 116, loss = 0.14124797\n",
      "Iteration 117, loss = 0.14148127\n",
      "Iteration 118, loss = 0.14245665\n",
      "Iteration 119, loss = 0.14045422\n",
      "Iteration 120, loss = 0.13734488\n",
      "Iteration 121, loss = 0.14095870\n",
      "Iteration 122, loss = 0.14046579\n",
      "Iteration 123, loss = 0.14123173\n",
      "Iteration 124, loss = 0.13832928\n",
      "Iteration 125, loss = 0.13998980\n",
      "Iteration 126, loss = 0.13856212\n",
      "Iteration 127, loss = 0.13902220\n",
      "Iteration 128, loss = 0.13567286\n",
      "Iteration 129, loss = 0.13672315\n",
      "Iteration 130, loss = 0.13643624\n",
      "Iteration 131, loss = 0.13521320\n",
      "Iteration 132, loss = 0.13427225\n",
      "Iteration 133, loss = 0.13532538\n",
      "Iteration 134, loss = 0.13407167\n",
      "Iteration 135, loss = 0.13432633\n",
      "Iteration 136, loss = 0.13276655\n",
      "Iteration 137, loss = 0.13289574\n",
      "Iteration 138, loss = 0.13251217\n",
      "Iteration 139, loss = 0.13170540\n",
      "Iteration 140, loss = 0.13182996\n",
      "Iteration 141, loss = 0.12942710\n",
      "Iteration 142, loss = 0.13213998\n",
      "Iteration 143, loss = 0.13249152\n",
      "Iteration 144, loss = 0.13293569\n",
      "Iteration 145, loss = 0.12978339\n",
      "Iteration 146, loss = 0.12846741\n",
      "Iteration 147, loss = 0.12985828\n",
      "Iteration 148, loss = 0.12701243\n",
      "Iteration 149, loss = 0.12633112\n",
      "Iteration 150, loss = 0.13039528\n",
      "Iteration 151, loss = 0.12942830\n",
      "Iteration 152, loss = 0.12808600\n",
      "Iteration 153, loss = 0.12644337\n",
      "Iteration 154, loss = 0.12698219\n",
      "Iteration 155, loss = 0.12706672\n",
      "Iteration 156, loss = 0.12556809\n",
      "Iteration 157, loss = 0.12386402\n",
      "Iteration 158, loss = 0.12495323\n",
      "Iteration 159, loss = 0.12358283\n",
      "Iteration 160, loss = 0.12466008\n",
      "Iteration 161, loss = 0.12615949\n",
      "Iteration 162, loss = 0.12413428\n",
      "Iteration 163, loss = 0.12447886\n",
      "Iteration 164, loss = 0.12288070\n",
      "Iteration 165, loss = 0.12294560\n",
      "Iteration 166, loss = 0.12356802\n",
      "Iteration 167, loss = 0.12439132\n",
      "Iteration 168, loss = 0.12201253\n",
      "Iteration 169, loss = 0.12149893\n",
      "Iteration 170, loss = 0.11990051\n",
      "Iteration 171, loss = 0.12169512\n",
      "Iteration 172, loss = 0.12055743\n",
      "Iteration 173, loss = 0.11875434\n",
      "Iteration 174, loss = 0.12044994\n",
      "Iteration 175, loss = 0.11950438\n",
      "Iteration 176, loss = 0.12127095\n",
      "Iteration 177, loss = 0.11805060\n",
      "Iteration 178, loss = 0.12070723\n",
      "Iteration 179, loss = 0.11767444\n",
      "Iteration 180, loss = 0.11935801\n",
      "Iteration 181, loss = 0.12062307\n",
      "Iteration 182, loss = 0.11790620\n",
      "Iteration 183, loss = 0.11878343\n",
      "Iteration 184, loss = 0.12339892\n",
      "Iteration 185, loss = 0.11875156\n",
      "Iteration 186, loss = 0.11994374\n",
      "Iteration 187, loss = 0.12124434\n",
      "Iteration 188, loss = 0.11529916\n",
      "Iteration 189, loss = 0.11752189\n",
      "Iteration 190, loss = 0.11507364\n",
      "Iteration 191, loss = 0.11559444\n",
      "Iteration 192, loss = 0.11519398\n",
      "Iteration 193, loss = 0.11514848\n",
      "Iteration 194, loss = 0.11663782\n",
      "Iteration 195, loss = 0.11289313\n",
      "Iteration 196, loss = 0.11666365\n",
      "Iteration 197, loss = 0.11262784\n",
      "Iteration 198, loss = 0.11371204\n",
      "Iteration 199, loss = 0.11313272\n",
      "Iteration 200, loss = 0.11692523\n",
      "Iteration 201, loss = 0.11339480\n",
      "Iteration 202, loss = 0.11169142\n",
      "Iteration 203, loss = 0.11345560\n",
      "Iteration 204, loss = 0.11241590\n",
      "Iteration 205, loss = 0.11211885\n",
      "Iteration 206, loss = 0.10981786\n",
      "Iteration 207, loss = 0.11106610\n",
      "Iteration 208, loss = 0.11095327\n",
      "Iteration 209, loss = 0.11180278\n",
      "Iteration 210, loss = 0.11311003\n",
      "Iteration 211, loss = 0.11036945\n",
      "Iteration 212, loss = 0.11159765\n",
      "Iteration 213, loss = 0.11070222\n",
      "Iteration 214, loss = 0.11040160\n",
      "Iteration 215, loss = 0.11059226\n",
      "Iteration 216, loss = 0.10892142\n",
      "Iteration 217, loss = 0.11045633\n",
      "Iteration 218, loss = 0.11351418\n",
      "Iteration 219, loss = 0.10865590\n",
      "Iteration 220, loss = 0.11097512\n",
      "Iteration 221, loss = 0.11150415\n",
      "Iteration 222, loss = 0.11057865\n",
      "Iteration 223, loss = 0.10551179\n",
      "Iteration 224, loss = 0.10712593\n",
      "Iteration 225, loss = 0.10763323\n",
      "Iteration 226, loss = 0.10994021\n",
      "Iteration 227, loss = 0.10900052\n",
      "Iteration 228, loss = 0.10761754\n",
      "Iteration 229, loss = 0.10785612\n",
      "Iteration 230, loss = 0.10858143\n",
      "Iteration 231, loss = 0.10546696\n",
      "Iteration 232, loss = 0.10894385\n",
      "Iteration 233, loss = 0.10737901\n",
      "Iteration 234, loss = 0.10313443\n",
      "Iteration 235, loss = 0.10627576\n",
      "Iteration 236, loss = 0.10378900\n",
      "Iteration 237, loss = 0.10651148\n",
      "Iteration 238, loss = 0.10732970\n",
      "Iteration 239, loss = 0.10323984\n",
      "Iteration 240, loss = 0.10440630\n",
      "Iteration 241, loss = 0.10516889\n",
      "Iteration 242, loss = 0.10303060\n",
      "Iteration 243, loss = 0.10461838\n",
      "Iteration 244, loss = 0.10366972\n",
      "Iteration 245, loss = 0.10616015\n",
      "Iteration 246, loss = 0.10747947\n",
      "Iteration 247, loss = 0.10978249\n",
      "Iteration 248, loss = 0.10688168\n",
      "Iteration 249, loss = 0.10444796\n",
      "Iteration 250, loss = 0.10262364\n",
      "Iteration 251, loss = 0.10386534\n",
      "Iteration 252, loss = inf\n",
      "Iteration 253, loss = 0.10362413\n",
      "Iteration 254, loss = 0.10119952\n",
      "Iteration 255, loss = 0.10066824\n",
      "Iteration 256, loss = 0.10139383\n",
      "Iteration 257, loss = 0.10074031\n",
      "Iteration 258, loss = 0.10059138\n",
      "Iteration 259, loss = 0.10095015\n",
      "Iteration 260, loss = 0.10246533\n",
      "Iteration 261, loss = 0.10150735\n",
      "Iteration 262, loss = 0.10232357\n",
      "Iteration 263, loss = 0.09996202\n",
      "Iteration 264, loss = 0.09852219\n",
      "Iteration 265, loss = 0.10224998\n",
      "Iteration 266, loss = 0.10339508\n",
      "Iteration 267, loss = 0.10270013\n",
      "Iteration 268, loss = 0.10373998\n",
      "Iteration 269, loss = 0.10339696\n",
      "Iteration 270, loss = 0.10073278\n",
      "Iteration 271, loss = 0.09928533\n",
      "Iteration 272, loss = 0.09776701\n",
      "Iteration 273, loss = 0.10004708\n",
      "Iteration 274, loss = 0.09909577\n",
      "Iteration 275, loss = 0.10138017\n",
      "Iteration 276, loss = 0.10056244\n",
      "Iteration 277, loss = 0.09794357\n",
      "Iteration 278, loss = 0.09678485\n",
      "Iteration 279, loss = 0.09889635\n",
      "Iteration 280, loss = 0.09769555\n",
      "Iteration 281, loss = 0.09742289\n",
      "Iteration 282, loss = 0.09736851\n",
      "Iteration 283, loss = 0.09680687\n",
      "Iteration 284, loss = 0.09730646\n",
      "Iteration 285, loss = 0.09786626\n",
      "Iteration 286, loss = 0.10012999\n",
      "Iteration 287, loss = 0.09688921\n",
      "Iteration 288, loss = 0.09794133\n",
      "Iteration 289, loss = 0.09985341\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100, 100), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=0, shuffle=True, solver='adam',\n",
       "              tol=1e-06, validation_fraction=0.1, verbose=True,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fazendo o treinamento com a base de dados de treinamento\n",
    "classificador.fit(previsores_treinamento, classe_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fazendo a predição usando a base de teste \n",
    "previsoes = classificador.predict(previsores_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' <=50K' ' <=50K' ' <=50K' ... ' <=50K' ' <=50K' ' >50K']\n"
     ]
    }
   ],
   "source": [
    "# observando os dados previstos com a base de teste\n",
    "print(previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' <=50K' ' <=50K' ' <=50K' ... ' <=50K' ' <=50K' ' <=50K']\n"
     ]
    }
   ],
   "source": [
    "# observando os resultados originais para a base de teste\n",
    "print(classe_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisando a capacidade de predição do algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando a biblioteca sklearn do python\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtendo a precisão do algoritmo\n",
    "precisao = accuracy_score(previsoes, classe_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8075742067553736\n"
     ]
    }
   ],
   "source": [
    "# observando a precisão de acertos do algoritmo\n",
    "print(precisao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A precisão de acertos do algoritmo foi de **80.76%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtendo a matriz de confusão que relaciona os acertos e erros do algoritmo\n",
    "matriz = confusion_matrix(previsoes, classe_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3274  521]\n",
      " [ 419  671]]\n"
     ]
    }
   ],
   "source": [
    "# observando a matriz de confusão para os dados previstos pelo algoritmo\n",
    "print(matriz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que 3274 dados referentes a pessoas que recebem menos que 50 mil por ano foram previstos corretamente, assim como 671 dados referentes a pessoas que recebem mais que 50 mil por ano. Entretanto, 521 dados referentes a pessoas que recebem menos que 50 mil anualmente foram classificados incorretamente como se recebessem mais e 419 dados referentes a pessoas que recebem mais que 50 mil por ano foram classificados incorretamente como se recebessem mais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pré-processamento: **LabelEncoder + StandardScaler**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando a biblioteca pandas do python\n",
    "import pandas as pd\n",
    "\n",
    "# atribuindo os registros da base de dados para o objeto \"dataframe\"\n",
    "dataframe = pd.read_csv('census.csv', encoding = 'utf-8', sep = ',')\n",
    "\n",
    "# separando os atributos previsores e classe \n",
    "previsores = dataframe.iloc[:, 0:14].values\n",
    "classe = dataframe.iloc[:, 14].values\n",
    "\n",
    "# importando a biblioteca sklearn do python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# LabelEncoder é uma função responsável por  normalizar rótulos\n",
    "# o objetivo é transoformar variáveis categóricas em numéricas\n",
    "\n",
    "# criando o objeto \"labelencoder_previsores\" para fazer a transformação dos atributos categóricos em numéricos\n",
    "labelencoder_previsores = LabelEncoder()\n",
    "\n",
    "# aplicando a transformação em todos os campos de dados categóricos usando o objeto criado e definido \n",
    "# \"LabelEncoder_previsores\"\n",
    "previsores[:,1] = labelencoder_previsores.fit_transform(previsores[:,1])\n",
    "previsores[:,3] = labelencoder_previsores.fit_transform(previsores[:,3])\n",
    "previsores[:,5] = labelencoder_previsores.fit_transform(previsores[:,5])\n",
    "previsores[:,6] = labelencoder_previsores.fit_transform(previsores[:,6])\n",
    "previsores[:,7] = labelencoder_previsores.fit_transform(previsores[:,7])\n",
    "previsores[:,8] = labelencoder_previsores.fit_transform(previsores[:,8])\n",
    "previsores[:,9] = labelencoder_previsores.fit_transform(previsores[:,9])\n",
    "previsores[:,13] = labelencoder_previsores.fit_transform(previsores[:,13])\n",
    "\n",
    "# importando a biblioteca sklearn do python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# criando e configurando o objeto \"scaler\"\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# aplicando o escaolonamento nos atributos previsores\n",
    "previsores = scaler.fit_transform(previsores)\n",
    "\n",
    "# importando a biblioteca sklearn do python\n",
    "from sklearn.model_selection import train_test_split\n",
    "# a função \"train_test_split\" tem a função de separar modelos de treinamento e modelos de teste em uma base \n",
    "#de dados\n",
    "\n",
    "# separando as bases de dados entre modelos de treinamento e modelos de teste\n",
    "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, \n",
    "                                                                                             classe,\n",
    "                                                                                             test_size = 0.15, \n",
    "                                                                                             random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando a biblioteca sklearn do python\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# 'MLPClassifier' é usado para criar uma rede neural\n",
    "# Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando e definindo um 'classificador' para definir os parâmetros da rede neural\n",
    "# verbose mostra na tela os dados em cada época de treinamento\n",
    "# max_iter = números de repetições, ou seja, quantidade de vezes que o algoritmo será treinado\n",
    "# tol = ajusta os pesos com passos pequenos\n",
    "# solver é a otimização utilizada, 'adam' é um melhoramente do método gradiente descente\n",
    "# activation = função de ativação\n",
    "# hidden_layer_sizes = quantidade de neurônios em cada camada oculta\n",
    "# learning_rate = indica o método de ajuste de pesos, em 'constant' o treinamento para quando o peso não \n",
    "# muda por duas vezes consecutivas e 'adaptative' muda a taxa de aprendizado quando o erro não diminui\n",
    "# muda a taxa de aprendizagem\n",
    "classificador = MLPClassifier(verbose = True, max_iter = 1000, tol = 0.0000010, solver = 'adam', \n",
    "                              hidden_layer_sizes = (100, 100), activation = 'relu', \n",
    "                              learning_rate = 'constant', random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.39965717\n",
      "Iteration 2, loss = 0.33080981\n",
      "Iteration 3, loss = 0.32301692\n",
      "Iteration 4, loss = 0.31983071\n",
      "Iteration 5, loss = 0.31787525\n",
      "Iteration 6, loss = 0.31522174\n",
      "Iteration 7, loss = 0.31433715\n",
      "Iteration 8, loss = 0.31312801\n",
      "Iteration 9, loss = 0.31165005\n",
      "Iteration 10, loss = 0.30955663\n",
      "Iteration 11, loss = 0.30787021\n",
      "Iteration 12, loss = 0.30728069\n",
      "Iteration 13, loss = 0.30626523\n",
      "Iteration 14, loss = 0.30505887\n",
      "Iteration 15, loss = 0.30314128\n",
      "Iteration 16, loss = 0.30311546\n",
      "Iteration 17, loss = 0.30133337\n",
      "Iteration 18, loss = 0.30038261\n",
      "Iteration 19, loss = 0.29949960\n",
      "Iteration 20, loss = 0.29751475\n",
      "Iteration 21, loss = 0.29708103\n",
      "Iteration 22, loss = 0.29582219\n",
      "Iteration 23, loss = 0.29625405\n",
      "Iteration 24, loss = 0.29385775\n",
      "Iteration 25, loss = 0.29300183\n",
      "Iteration 26, loss = 0.29149857\n",
      "Iteration 27, loss = 0.29120671\n",
      "Iteration 28, loss = 0.29034995\n",
      "Iteration 29, loss = 0.28916598\n",
      "Iteration 30, loss = 0.28772804\n",
      "Iteration 31, loss = 0.28725665\n",
      "Iteration 32, loss = 0.28623382\n",
      "Iteration 33, loss = 0.28495584\n",
      "Iteration 34, loss = 0.28551720\n",
      "Iteration 35, loss = 0.28464496\n",
      "Iteration 36, loss = 0.28263215\n",
      "Iteration 37, loss = 0.28236551\n",
      "Iteration 38, loss = 0.28214317\n",
      "Iteration 39, loss = 0.28063684\n",
      "Iteration 40, loss = 0.27950796\n",
      "Iteration 41, loss = 0.27887436\n",
      "Iteration 42, loss = 0.27768658\n",
      "Iteration 43, loss = 0.27677646\n",
      "Iteration 44, loss = 0.27567999\n",
      "Iteration 45, loss = 0.27545780\n",
      "Iteration 46, loss = 0.27565757\n",
      "Iteration 47, loss = 0.27503420\n",
      "Iteration 48, loss = 0.27344690\n",
      "Iteration 49, loss = 0.27281240\n",
      "Iteration 50, loss = 0.27103676\n",
      "Iteration 51, loss = 0.27065758\n",
      "Iteration 52, loss = 0.27040730\n",
      "Iteration 53, loss = 0.27108477\n",
      "Iteration 54, loss = 0.26826168\n",
      "Iteration 55, loss = 0.26913119\n",
      "Iteration 56, loss = 0.26724153\n",
      "Iteration 57, loss = 0.26672465\n",
      "Iteration 58, loss = 0.26684681\n",
      "Iteration 59, loss = 0.26466788\n",
      "Iteration 60, loss = 0.26501975\n",
      "Iteration 61, loss = 0.26355079\n",
      "Iteration 62, loss = 0.26389011\n",
      "Iteration 63, loss = 0.26220287\n",
      "Iteration 64, loss = 0.26114347\n",
      "Iteration 65, loss = 0.26109048\n",
      "Iteration 66, loss = 0.26096102\n",
      "Iteration 67, loss = 0.25948083\n",
      "Iteration 68, loss = 0.25911665\n",
      "Iteration 69, loss = 0.25842294\n",
      "Iteration 70, loss = 0.25768873\n",
      "Iteration 71, loss = 0.25618717\n",
      "Iteration 72, loss = 0.25651875\n",
      "Iteration 73, loss = 0.25504917\n",
      "Iteration 74, loss = 0.25477684\n",
      "Iteration 75, loss = 0.25440807\n",
      "Iteration 76, loss = 0.25298408\n",
      "Iteration 77, loss = 0.25262267\n",
      "Iteration 78, loss = 0.25195753\n",
      "Iteration 79, loss = 0.25113691\n",
      "Iteration 80, loss = 0.25136534\n",
      "Iteration 81, loss = 0.25005022\n",
      "Iteration 82, loss = 0.25019655\n",
      "Iteration 83, loss = 0.24858762\n",
      "Iteration 84, loss = 0.24922695\n",
      "Iteration 85, loss = 0.24778355\n",
      "Iteration 86, loss = 0.24626691\n",
      "Iteration 87, loss = 0.24662462\n",
      "Iteration 88, loss = 0.24565203\n",
      "Iteration 89, loss = 0.24552690\n",
      "Iteration 90, loss = 0.24721272\n",
      "Iteration 91, loss = 0.24410149\n",
      "Iteration 92, loss = 0.24634817\n",
      "Iteration 93, loss = 0.24310296\n",
      "Iteration 94, loss = 0.24218369\n",
      "Iteration 95, loss = 0.24174364\n",
      "Iteration 96, loss = 0.24070508\n",
      "Iteration 97, loss = 0.23987174\n",
      "Iteration 98, loss = 0.24037400\n",
      "Iteration 99, loss = 0.23966153\n",
      "Iteration 100, loss = 0.23931356\n",
      "Iteration 101, loss = 0.23901254\n",
      "Iteration 102, loss = 0.23615851\n",
      "Iteration 103, loss = 0.23774151\n",
      "Iteration 104, loss = 0.23895634\n",
      "Iteration 105, loss = 0.23624902\n",
      "Iteration 106, loss = 0.23583344\n",
      "Iteration 107, loss = 0.23350809\n",
      "Iteration 108, loss = 0.23441243\n",
      "Iteration 109, loss = 0.23481051\n",
      "Iteration 110, loss = 0.23301018\n",
      "Iteration 111, loss = 0.23118203\n",
      "Iteration 112, loss = 0.23212492\n",
      "Iteration 113, loss = 0.23075956\n",
      "Iteration 114, loss = 0.23046297\n",
      "Iteration 115, loss = 0.23000629\n",
      "Iteration 116, loss = 0.23020622\n",
      "Iteration 117, loss = 0.23056057\n",
      "Iteration 118, loss = 0.23088947\n",
      "Iteration 119, loss = 0.22837449\n",
      "Iteration 120, loss = 0.22872987\n",
      "Iteration 121, loss = 0.22558039\n",
      "Iteration 122, loss = 0.22663161\n",
      "Iteration 123, loss = 0.22590671\n",
      "Iteration 124, loss = 0.22666232\n",
      "Iteration 125, loss = 0.22653797\n",
      "Iteration 126, loss = 0.22330931\n",
      "Iteration 127, loss = 0.22303628\n",
      "Iteration 128, loss = 0.22341010\n",
      "Iteration 129, loss = 0.22189887\n",
      "Iteration 130, loss = 0.22114915\n",
      "Iteration 131, loss = 0.22085612\n",
      "Iteration 132, loss = 0.22173501\n",
      "Iteration 133, loss = 0.22093862\n",
      "Iteration 134, loss = 0.22087359\n",
      "Iteration 135, loss = 0.21972449\n",
      "Iteration 136, loss = 0.22060442\n",
      "Iteration 137, loss = 0.22047204\n",
      "Iteration 138, loss = 0.22038809\n",
      "Iteration 139, loss = 0.21727558\n",
      "Iteration 140, loss = 0.21780301\n",
      "Iteration 141, loss = 0.21711692\n",
      "Iteration 142, loss = 0.21675210\n",
      "Iteration 143, loss = 0.21416913\n",
      "Iteration 144, loss = 0.21417003\n",
      "Iteration 145, loss = 0.21528371\n",
      "Iteration 146, loss = 0.21328856\n",
      "Iteration 147, loss = 0.21420928\n",
      "Iteration 148, loss = 0.21192491\n",
      "Iteration 149, loss = 0.21388743\n",
      "Iteration 150, loss = 0.21421463\n",
      "Iteration 151, loss = 0.21128160\n",
      "Iteration 152, loss = 0.21104655\n",
      "Iteration 153, loss = 0.21100067\n",
      "Iteration 154, loss = 0.21074368\n",
      "Iteration 155, loss = 0.21161213\n",
      "Iteration 156, loss = 0.20958132\n",
      "Iteration 157, loss = 0.21005306\n",
      "Iteration 158, loss = 0.20847706\n",
      "Iteration 159, loss = 0.20787704\n",
      "Iteration 160, loss = 0.20661655\n",
      "Iteration 161, loss = 0.20725111\n",
      "Iteration 162, loss = 0.20691092\n",
      "Iteration 163, loss = 0.20670170\n",
      "Iteration 164, loss = 0.20809922\n",
      "Iteration 165, loss = 0.20388258\n",
      "Iteration 166, loss = 0.20558635\n",
      "Iteration 167, loss = 0.20623643\n",
      "Iteration 168, loss = 0.20289231\n",
      "Iteration 169, loss = 0.20330247\n",
      "Iteration 170, loss = 0.20273539\n",
      "Iteration 171, loss = 0.20248592\n",
      "Iteration 172, loss = 0.20357293\n",
      "Iteration 173, loss = 0.20300856\n",
      "Iteration 174, loss = 0.20245873\n",
      "Iteration 175, loss = 0.20190700\n",
      "Iteration 176, loss = 0.20077456\n",
      "Iteration 177, loss = 0.20179339\n",
      "Iteration 178, loss = 0.19960921\n",
      "Iteration 179, loss = 0.19908869\n",
      "Iteration 180, loss = 0.19931371\n",
      "Iteration 181, loss = 0.19818149\n",
      "Iteration 182, loss = 0.19818665\n",
      "Iteration 183, loss = 0.19680643\n",
      "Iteration 184, loss = 0.19872037\n",
      "Iteration 185, loss = 0.19844852\n",
      "Iteration 186, loss = 0.19632460\n",
      "Iteration 187, loss = 0.19936404\n",
      "Iteration 188, loss = 0.19703535\n",
      "Iteration 189, loss = 0.19828174\n",
      "Iteration 190, loss = 0.19329617\n",
      "Iteration 191, loss = 0.19455637\n",
      "Iteration 192, loss = 0.19425002\n",
      "Iteration 193, loss = 0.19317861\n",
      "Iteration 194, loss = 0.19415031\n",
      "Iteration 195, loss = 0.19378444\n",
      "Iteration 196, loss = 0.19248291\n",
      "Iteration 197, loss = 0.19440492\n",
      "Iteration 198, loss = 0.19206652\n",
      "Iteration 199, loss = 0.19216225\n",
      "Iteration 200, loss = 0.19063690\n",
      "Iteration 201, loss = 0.19071503\n",
      "Iteration 202, loss = 0.19156469\n",
      "Iteration 203, loss = 0.19019897\n",
      "Iteration 204, loss = 0.19034209\n",
      "Iteration 205, loss = 0.19004775\n",
      "Iteration 206, loss = 0.18784992\n",
      "Iteration 207, loss = 0.18847912\n",
      "Iteration 208, loss = 0.18835624\n",
      "Iteration 209, loss = 0.19073142\n",
      "Iteration 210, loss = 0.18821412\n",
      "Iteration 211, loss = 0.18749508\n",
      "Iteration 212, loss = 0.18920978\n",
      "Iteration 213, loss = 0.18722575\n",
      "Iteration 214, loss = 0.18854673\n",
      "Iteration 215, loss = 0.18543437\n",
      "Iteration 216, loss = 0.18419816\n",
      "Iteration 217, loss = 0.18519626\n",
      "Iteration 218, loss = 0.18529928\n",
      "Iteration 219, loss = 0.18518258\n",
      "Iteration 220, loss = 0.18485136\n",
      "Iteration 221, loss = 0.18625979\n",
      "Iteration 222, loss = 0.18634117\n",
      "Iteration 223, loss = 0.18229265\n",
      "Iteration 224, loss = 0.18405872\n",
      "Iteration 225, loss = 0.18337532\n",
      "Iteration 226, loss = 0.18128977\n",
      "Iteration 227, loss = 0.18356102\n",
      "Iteration 228, loss = 0.18285804\n",
      "Iteration 229, loss = 0.18119952\n",
      "Iteration 230, loss = 0.18220155\n",
      "Iteration 231, loss = 0.18183160\n",
      "Iteration 232, loss = 0.18010601\n",
      "Iteration 233, loss = 0.18050076\n",
      "Iteration 234, loss = 0.18022271\n",
      "Iteration 235, loss = 0.17776089\n",
      "Iteration 236, loss = 0.17877759\n",
      "Iteration 237, loss = 0.18034717\n",
      "Iteration 238, loss = 0.17818799\n",
      "Iteration 239, loss = 0.18129981\n",
      "Iteration 240, loss = 0.17703068\n",
      "Iteration 241, loss = 0.17809030\n",
      "Iteration 242, loss = 0.17790134\n",
      "Iteration 243, loss = 0.17778586\n",
      "Iteration 244, loss = 0.17519416\n",
      "Iteration 245, loss = 0.17845079\n",
      "Iteration 246, loss = 0.17541683\n",
      "Iteration 247, loss = 0.18018313\n",
      "Iteration 248, loss = 0.17597688\n",
      "Iteration 249, loss = 0.17591435\n",
      "Iteration 250, loss = 0.17619289\n",
      "Iteration 251, loss = 0.17457233\n",
      "Iteration 252, loss = 0.17437426\n",
      "Iteration 253, loss = 0.17372980\n",
      "Iteration 254, loss = 0.17400800\n",
      "Iteration 255, loss = 0.17617927\n",
      "Iteration 256, loss = 0.17242095\n",
      "Iteration 257, loss = 0.17312796\n",
      "Iteration 258, loss = 0.17361550\n",
      "Iteration 259, loss = 0.17389262\n",
      "Iteration 260, loss = 0.17196663\n",
      "Iteration 261, loss = 0.17108701\n",
      "Iteration 262, loss = 0.17170763\n",
      "Iteration 263, loss = 0.17166400\n",
      "Iteration 264, loss = 0.17212855\n",
      "Iteration 265, loss = 0.17018773\n",
      "Iteration 266, loss = 0.17130498\n",
      "Iteration 267, loss = 0.17152920\n",
      "Iteration 268, loss = 0.17067166\n",
      "Iteration 269, loss = 0.17108787\n",
      "Iteration 270, loss = 0.16882062\n",
      "Iteration 271, loss = 0.16674614\n",
      "Iteration 272, loss = 0.17004584\n",
      "Iteration 273, loss = 0.16874153\n",
      "Iteration 274, loss = 0.16876628\n",
      "Iteration 275, loss = 0.16819676\n",
      "Iteration 276, loss = 0.16947764\n",
      "Iteration 277, loss = 0.16841828\n",
      "Iteration 278, loss = 0.16761223\n",
      "Iteration 279, loss = 0.16699395\n",
      "Iteration 280, loss = 0.16866318\n",
      "Iteration 281, loss = 0.16614552\n",
      "Iteration 282, loss = 0.16520666\n",
      "Iteration 283, loss = 0.16790989\n",
      "Iteration 284, loss = 0.16530104\n",
      "Iteration 285, loss = 0.16698105\n",
      "Iteration 286, loss = 0.16514678\n",
      "Iteration 287, loss = 0.16516139\n",
      "Iteration 288, loss = 0.16553068\n",
      "Iteration 289, loss = 0.16307097\n",
      "Iteration 290, loss = 0.16271787\n",
      "Iteration 291, loss = 0.16448696\n",
      "Iteration 292, loss = 0.16388243\n",
      "Iteration 293, loss = 0.16589282\n",
      "Iteration 294, loss = 0.16249333\n",
      "Iteration 295, loss = 0.16205425\n",
      "Iteration 296, loss = 0.16358292\n",
      "Iteration 297, loss = 0.16455342\n",
      "Iteration 298, loss = 0.16414845\n",
      "Iteration 299, loss = 0.16096199\n",
      "Iteration 300, loss = 0.16147659\n",
      "Iteration 301, loss = 0.16210949\n",
      "Iteration 302, loss = 0.16210724\n",
      "Iteration 303, loss = 0.16045787\n",
      "Iteration 304, loss = 0.16123774\n",
      "Iteration 305, loss = 0.16030938\n",
      "Iteration 306, loss = 0.16021190\n",
      "Iteration 307, loss = 0.16012400\n",
      "Iteration 308, loss = 0.15963730\n",
      "Iteration 309, loss = 0.16273362\n",
      "Iteration 310, loss = 0.16104889\n",
      "Iteration 311, loss = 0.15740554\n",
      "Iteration 312, loss = 0.15997421\n",
      "Iteration 313, loss = 0.15936692\n",
      "Iteration 314, loss = 0.15813259\n",
      "Iteration 315, loss = 0.16066709\n",
      "Iteration 316, loss = 0.15882808\n",
      "Iteration 317, loss = 0.15883470\n",
      "Iteration 318, loss = 0.15799438\n",
      "Iteration 319, loss = 0.15588896\n",
      "Iteration 320, loss = 0.15565331\n",
      "Iteration 321, loss = 0.15568426\n",
      "Iteration 322, loss = 0.15766776\n",
      "Iteration 323, loss = 0.15751023\n",
      "Iteration 324, loss = 0.15595272\n",
      "Iteration 325, loss = 0.15534687\n",
      "Iteration 326, loss = 0.15728650\n",
      "Iteration 327, loss = 0.15797277\n",
      "Iteration 328, loss = 0.15554536\n",
      "Iteration 329, loss = 0.15546899\n",
      "Iteration 330, loss = 0.15289744\n",
      "Iteration 331, loss = 0.15434144\n",
      "Iteration 332, loss = 0.15381252\n",
      "Iteration 333, loss = 0.15623337\n",
      "Iteration 334, loss = 0.15702459\n",
      "Iteration 335, loss = 0.15569984\n",
      "Iteration 336, loss = 0.15426502\n",
      "Iteration 337, loss = 0.15163308\n",
      "Iteration 338, loss = 0.15353041\n",
      "Iteration 339, loss = 0.15446953\n",
      "Iteration 340, loss = 0.15199842\n",
      "Iteration 341, loss = 0.15364736\n",
      "Iteration 342, loss = 0.15342888\n",
      "Iteration 343, loss = 0.15216268\n",
      "Iteration 344, loss = 0.15188912\n",
      "Iteration 345, loss = 0.15267222\n",
      "Iteration 346, loss = 0.15362863\n",
      "Iteration 347, loss = 0.15221229\n",
      "Iteration 348, loss = 0.15400487\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100, 100), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=0, shuffle=True, solver='adam',\n",
       "              tol=1e-06, validation_fraction=0.1, verbose=True,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fazendo o treinamento com a base de dados de treinamento\n",
    "classificador.fit(previsores_treinamento, classe_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fazendo a predição usando a base de teste \n",
    "previsoes = classificador.predict(previsores_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' <=50K' ' <=50K' ' <=50K' ... ' <=50K' ' <=50K' ' >50K']\n"
     ]
    }
   ],
   "source": [
    "# observando os dados previstos com a base de teste\n",
    "print(previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' <=50K' ' <=50K' ' <=50K' ... ' <=50K' ' <=50K' ' <=50K']\n"
     ]
    }
   ],
   "source": [
    "# observando os resultados originais para a base de teste\n",
    "print(classe_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisando a capacidade de predição do algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando a biblioteca sklearn do python\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtendo a precisão do algoritmo\n",
    "precisao = accuracy_score(previsoes, classe_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8106448311156602\n"
     ]
    }
   ],
   "source": [
    "# observando a precisão de acertos do algoritmo\n",
    "print(precisao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A precisão do algoritmo foi de **81.06%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtendo a matriz de confusão que relaciona os acertos e erros do algoritmo\n",
    "matriz = confusion_matrix(previsoes, classe_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3218  450]\n",
      " [ 475  742]]\n"
     ]
    }
   ],
   "source": [
    "# observando a matriz de confusão para os dados previstos pelo algoritmo\n",
    "print(matriz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que 3218 dados referentes a pessoas que recebem menos que 50 mil por ano foram previstos corretamente, assim como 742 dados referentes a pessoas que recebem mais que 50 mil por ano. Entretanto, 450 dados referentes a pessoas que recebem menos que 50 mil anualmente foram classificados incorretamente como se recebessem mais e 475 dados referentes a pessoas que recebem mais que 50 mil por ano foram classificados incorretamente como se recebessem mais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observando o balanceamento das classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando a biblioteca collections do python\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({' <=50K': 3693, ' >50K': 1192})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualizando a quantidade de registros para cada uma das classes\n",
    "collections.Counter(classe_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alguma dúvida? Entre em contato comigo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Me envie um e-mail](mailto:alyssonmachado388@gmail.com);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
